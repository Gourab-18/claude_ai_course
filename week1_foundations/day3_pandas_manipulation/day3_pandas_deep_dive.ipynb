{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3: Pandas Data Manipulation\n",
    "\n",
    "Pandas is the powerhouse library for data manipulation in Python. Today we'll master:\n",
    "\n",
    "1. **DataFrames**: Creation, indexing, and filtering\n",
    "2. **Handling Missing Data**: dropna, fillna, interpolation\n",
    "3. **GroupBy Operations**: Aggregations and transformations\n",
    "4. **Merging and Joining**: Combining datasets\n",
    "5. **Assignment**: Clean a messy dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 200)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(\"Ready to learn Pandas!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. DataFrames: Creation, Indexing, and Filtering\n",
    "\n",
    "### 1.1 Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: From a dictionary\n",
    "data = {\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'Age': [25, 30, 35, 28, 32],\n",
    "    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'],\n",
    "    'Salary': [70000, 80000, 90000, 75000, 85000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"DataFrame from dictionary:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: From a list of dictionaries\n",
    "records = [\n",
    "    {'Name': 'Frank', 'Age': 40, 'City': 'Boston', 'Salary': 95000},\n",
    "    {'Name': 'Grace', 'Age': 27, 'City': 'Seattle', 'Salary': 72000},\n",
    "    {'Name': 'Henry', 'Age': 33, 'City': 'Denver', 'Salary': 88000}\n",
    "]\n",
    "\n",
    "df_records = pd.DataFrame(records)\n",
    "print(\"DataFrame from list of dictionaries:\")\n",
    "print(df_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: From NumPy array\n",
    "np_data = np.random.randint(1, 100, (5, 4))\n",
    "df_numpy = pd.DataFrame(\n",
    "    np_data,\n",
    "    columns=['A', 'B', 'C', 'D'],\n",
    "    index=['row1', 'row2', 'row3', 'row4', 'row5']\n",
    ")\n",
    "print(\"DataFrame from NumPy array:\")\n",
    "print(df_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading built-in datasets\n",
    "titanic = sns.load_dataset('titanic')\n",
    "tips = sns.load_dataset('tips')\n",
    "iris = sns.load_dataset('iris')\n",
    "\n",
    "print(\"Titanic dataset shape:\", titanic.shape)\n",
    "print(\"Tips dataset shape:\", tips.shape)\n",
    "print(\"Iris dataset shape:\", iris.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 DataFrame Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic exploration methods\n",
    "print(\"First 5 rows:\")\n",
    "print(titanic.head())\n",
    "\n",
    "print(\"\\nLast 5 rows:\")\n",
    "print(titanic.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame info\n",
    "print(\"DataFrame Info:\")\n",
    "print(titanic.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape, columns, and index\n",
    "print(f\"Shape: {titanic.shape}\")\n",
    "print(f\"\\nColumns: {list(titanic.columns)}\")\n",
    "print(f\"\\nIndex: {titanic.index}\")\n",
    "print(f\"\\nData types:\\n{titanic.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print(\"Statistical Summary (numerical columns):\")\n",
    "print(titanic.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary for all columns including categorical\n",
    "print(\"Summary (all columns):\")\n",
    "print(titanic.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Indexing and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting columns\n",
    "print(\"Single column (returns Series):\")\n",
    "print(titanic['age'].head())\n",
    "\n",
    "print(\"\\nMultiple columns (returns DataFrame):\")\n",
    "print(titanic[['age', 'sex', 'survived']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc - Label-based indexing\n",
    "print(\"Using loc (label-based):\")\n",
    "print(\"\\nRow 0:\")\n",
    "print(titanic.loc[0])\n",
    "\n",
    "print(\"\\nRows 0-4, specific columns:\")\n",
    "print(titanic.loc[0:4, ['name', 'age', 'sex']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iloc - Integer-based indexing\n",
    "print(\"Using iloc (integer-based):\")\n",
    "print(\"\\nFirst 3 rows, first 4 columns:\")\n",
    "print(titanic.iloc[:3, :4])\n",
    "\n",
    "print(\"\\nSpecific rows and columns:\")\n",
    "print(titanic.iloc[[0, 5, 10], [1, 2, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single value access\n",
    "print(f\"Value at row 0, column 'age': {titanic.loc[0, 'age']}\")\n",
    "print(f\"Value at row 0, column 3: {titanic.iloc[0, 3]}\")\n",
    "\n",
    "# Using at and iat for faster scalar access\n",
    "print(f\"Using at: {titanic.at[0, 'age']}\")\n",
    "print(f\"Using iat: {titanic.iat[0, 3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Filtering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean filtering\n",
    "print(\"Passengers older than 50:\")\n",
    "print(titanic[titanic['age'] > 50].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple conditions\n",
    "# Use & for AND, | for OR, ~ for NOT\n",
    "# Always wrap conditions in parentheses\n",
    "\n",
    "# Female passengers who survived\n",
    "female_survived = titanic[(titanic['sex'] == 'female') & (titanic['survived'] == 1)]\n",
    "print(f\"Female survivors: {len(female_survived)}\")\n",
    "print(female_survived.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First class OR second class passengers\n",
    "upper_class = titanic[(titanic['pclass'] == 1) | (titanic['pclass'] == 2)]\n",
    "print(f\"First or Second class passengers: {len(upper_class)}\")\n",
    "\n",
    "# Alternative using isin()\n",
    "upper_class_alt = titanic[titanic['pclass'].isin([1, 2])]\n",
    "print(f\"Using isin(): {len(upper_class_alt)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT condition\n",
    "not_third_class = titanic[~(titanic['pclass'] == 3)]\n",
    "print(f\"Not third class: {len(not_third_class)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using query() method - cleaner syntax\n",
    "result = titanic.query('age > 30 and sex == \"male\" and survived == 1')\n",
    "print(f\"Male survivors over 30: {len(result)}\")\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String filtering\n",
    "# Names containing 'Mrs'\n",
    "mrs_passengers = titanic[titanic['who'] == 'woman']\n",
    "print(f\"Women passengers: {len(mrs_passengers)}\")\n",
    "\n",
    "# Using str accessor for string operations\n",
    "# (Note: 'name' column doesn't exist in seaborn's titanic, this is illustrative)\n",
    "print(\"\\nClass distribution:\")\n",
    "print(titanic['class'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Between filter\n",
    "age_between = titanic[titanic['age'].between(20, 30)]\n",
    "print(f\"Passengers aged 20-30: {len(age_between)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Handling Missing Data\n",
    "\n",
    "Missing data is common in real-world datasets. Pandas provides several methods to handle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(titanic.isnull().sum())\n",
    "\n",
    "print(f\"\\nTotal missing values: {titanic.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value percentage\n",
    "missing_percent = (titanic.isnull().sum() / len(titanic) * 100).round(2)\n",
    "print(\"Missing value percentage:\")\n",
    "print(missing_percent[missing_percent > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(titanic.isnull(), cbar=True, yticklabels=False, cmap='viridis')\n",
    "plt.title('Missing Values Heatmap', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dropping Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame with missing values\n",
    "df_missing = pd.DataFrame({\n",
    "    'A': [1, 2, np.nan, 4, 5],\n",
    "    'B': [np.nan, 2, 3, np.nan, 5],\n",
    "    'C': [1, 2, 3, 4, 5],\n",
    "    'D': [np.nan, np.nan, np.nan, np.nan, 5]\n",
    "})\n",
    "print(\"Original DataFrame:\")\n",
    "print(df_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropna - remove rows with any missing values\n",
    "print(\"After dropna() - remove rows with any NaN:\")\n",
    "print(df_missing.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows only if ALL values are missing\n",
    "print(\"Drop rows where ALL values are NaN:\")\n",
    "print(df_missing.dropna(how='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with missing values\n",
    "print(\"Drop columns with any NaN:\")\n",
    "print(df_missing.dropna(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in specific columns\n",
    "print(\"Drop rows with NaN in column 'A':\")\n",
    "print(df_missing.dropna(subset=['A']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh - keep rows with at least n non-null values\n",
    "print(\"Keep rows with at least 3 non-null values:\")\n",
    "print(df_missing.dropna(thresh=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna with a constant value\n",
    "print(\"Fill with 0:\")\n",
    "print(df_missing.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with different values per column\n",
    "fill_values = {'A': 0, 'B': 999, 'D': -1}\n",
    "print(\"Fill with different values per column:\")\n",
    "print(df_missing.fillna(fill_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill with mean/median/mode\n",
    "df_missing_copy = df_missing.copy()\n",
    "\n",
    "# Fill column A with mean\n",
    "df_missing_copy['A'] = df_missing_copy['A'].fillna(df_missing_copy['A'].mean())\n",
    "print(f\"Column A mean: {df_missing['A'].mean():.2f}\")\n",
    "print(\"\\nAfter filling A with mean:\")\n",
    "print(df_missing_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward fill (ffill) - use previous value\n",
    "print(\"Forward fill:\")\n",
    "print(df_missing.ffill())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward fill (bfill) - use next value\n",
    "print(\"Backward fill:\")\n",
    "print(df_missing.bfill())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series-like data\n",
    "ts_data = pd.DataFrame({\n",
    "    'value': [1, np.nan, np.nan, 4, 5, np.nan, 7, 8, np.nan, 10]\n",
    "})\n",
    "print(\"Original:\")\n",
    "print(ts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear interpolation\n",
    "print(\"Linear interpolation:\")\n",
    "print(ts_data.interpolate(method='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize interpolation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Original\n",
    "axes[0].plot(ts_data['value'], 'bo-', markersize=8)\n",
    "axes[0].set_title('Original (with NaN)')\n",
    "axes[0].set_ylabel('Value')\n",
    "\n",
    "# Linear interpolation\n",
    "linear_filled = ts_data.interpolate(method='linear')\n",
    "axes[1].plot(linear_filled['value'], 'go-', markersize=8)\n",
    "axes[1].set_title('Linear Interpolation')\n",
    "\n",
    "# Polynomial interpolation\n",
    "poly_filled = ts_data.interpolate(method='polynomial', order=2)\n",
    "axes[2].plot(poly_filled['value'], 'ro-', markersize=8)\n",
    "axes[2].set_title('Polynomial Interpolation (order=2)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Practical Example: Handling Missing Values in Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy titanic for manipulation\n",
    "titanic_clean = titanic.copy()\n",
    "\n",
    "print(\"Missing values before cleaning:\")\n",
    "print(titanic_clean.isnull().sum()[titanic_clean.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 1: Fill age with median (robust to outliers)\n",
    "titanic_clean['age'] = titanic_clean['age'].fillna(titanic_clean['age'].median())\n",
    "\n",
    "print(f\"Age median: {titanic['age'].median():.1f}\")\n",
    "print(f\"Age missing after fill: {titanic_clean['age'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 2: Fill embarked with mode (most frequent)\n",
    "titanic_clean['embarked'] = titanic_clean['embarked'].fillna(titanic_clean['embarked'].mode()[0])\n",
    "titanic_clean['embark_town'] = titanic_clean['embark_town'].fillna(titanic_clean['embark_town'].mode()[0])\n",
    "\n",
    "print(f\"Embarked mode: {titanic['embarked'].mode()[0]}\")\n",
    "print(f\"Embarked missing after fill: {titanic_clean['embarked'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy 3: Drop deck column (too many missing values - 77%)\n",
    "titanic_clean = titanic_clean.drop('deck', axis=1)\n",
    "\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(titanic_clean.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. GroupBy Operations\n",
    "\n",
    "GroupBy allows you to split data, apply functions, and combine results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Basic GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by single column\n",
    "grouped = titanic.groupby('sex')\n",
    "print(f\"GroupBy object: {type(grouped)}\")\n",
    "print(f\"Number of groups: {grouped.ngroups}\")\n",
    "print(f\"Groups: {list(grouped.groups.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of all numeric columns by sex\n",
    "print(\"Mean by sex:\")\n",
    "print(titanic.groupby('sex')[['age', 'fare', 'survived']].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by multiple columns\n",
    "print(\"Survival rate by sex and class:\")\n",
    "print(titanic.groupby(['sex', 'pclass'])['survived'].mean().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstack for better readability\n",
    "print(\"Survival rate (unstacked):\")\n",
    "print(titanic.groupby(['sex', 'pclass'])['survived'].mean().unstack().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple aggregations\n",
    "print(\"Multiple aggregations:\")\n",
    "print(titanic.groupby('pclass')['fare'].agg(['count', 'mean', 'median', 'std', 'min', 'max']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different aggregations for different columns\n",
    "agg_dict = {\n",
    "    'age': ['mean', 'median'],\n",
    "    'fare': ['mean', 'sum'],\n",
    "    'survived': ['sum', 'mean']\n",
    "}\n",
    "\n",
    "print(\"Different aggregations per column:\")\n",
    "result = titanic.groupby('pclass').agg(agg_dict)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named aggregations (cleaner output)\n",
    "result = titanic.groupby('pclass').agg(\n",
    "    avg_age=('age', 'mean'),\n",
    "    median_age=('age', 'median'),\n",
    "    total_fare=('fare', 'sum'),\n",
    "    survival_rate=('survived', 'mean'),\n",
    "    passenger_count=('survived', 'count')\n",
    ")\n",
    "\n",
    "print(\"Named aggregations:\")\n",
    "print(result.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom aggregation functions\n",
    "def range_calc(x):\n",
    "    return x.max() - x.min()\n",
    "\n",
    "def coef_variation(x):\n",
    "    return x.std() / x.mean() * 100\n",
    "\n",
    "print(\"Custom aggregations:\")\n",
    "custom_agg = titanic.groupby('pclass')['fare'].agg(['mean', range_calc, coef_variation])\n",
    "custom_agg.columns = ['Mean', 'Range', 'CV%']\n",
    "print(custom_agg.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform returns data in the same shape as input\n",
    "# Useful for creating new columns based on group statistics\n",
    "\n",
    "# Create a copy for demonstration\n",
    "df_transform = titanic[['pclass', 'age', 'fare']].copy()\n",
    "\n",
    "# Add group mean as new column\n",
    "df_transform['fare_group_mean'] = df_transform.groupby('pclass')['fare'].transform('mean')\n",
    "\n",
    "# Calculate deviation from group mean\n",
    "df_transform['fare_deviation'] = df_transform['fare'] - df_transform['fare_group_mean']\n",
    "\n",
    "print(\"Transform example:\")\n",
    "print(df_transform.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize within groups (z-score per group)\n",
    "def zscore(x):\n",
    "    return (x - x.mean()) / x.std()\n",
    "\n",
    "df_transform['fare_zscore'] = df_transform.groupby('pclass')['fare'].transform(zscore)\n",
    "print(\"\\nZ-score normalized fare within groups:\")\n",
    "print(df_transform[['pclass', 'fare', 'fare_zscore']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter groups based on group properties\n",
    "# Keep only embarkation towns with more than 100 passengers\n",
    "\n",
    "filtered = titanic.groupby('embark_town').filter(lambda x: len(x) > 100)\n",
    "print(f\"Original rows: {len(titanic)}\")\n",
    "print(f\"After filtering: {len(filtered)}\")\n",
    "print(f\"\\nRemaining embark towns: {filtered['embark_town'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter groups where mean survival rate is above 0.5\n",
    "high_survival = titanic.groupby('pclass').filter(lambda x: x['survived'].mean() > 0.5)\n",
    "print(f\"Classes with survival rate > 50%:\")\n",
    "print(high_survival['pclass'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Merging and Joining DataFrames\n",
    "\n",
    "Combining data from multiple sources is essential in data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample DataFrames\n",
    "employees = pd.DataFrame({\n",
    "    'emp_id': [1, 2, 3, 4, 5],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'],\n",
    "    'dept_id': [101, 102, 101, 103, 102]\n",
    "})\n",
    "\n",
    "departments = pd.DataFrame({\n",
    "    'dept_id': [101, 102, 103, 104],\n",
    "    'dept_name': ['Engineering', 'Marketing', 'Sales', 'HR']\n",
    "})\n",
    "\n",
    "salaries = pd.DataFrame({\n",
    "    'emp_id': [1, 2, 3, 6],\n",
    "    'salary': [70000, 65000, 80000, 75000]\n",
    "})\n",
    "\n",
    "print(\"Employees:\")\n",
    "print(employees)\n",
    "print(\"\\nDepartments:\")\n",
    "print(departments)\n",
    "print(\"\\nSalaries:\")\n",
    "print(salaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Merge (SQL-like joins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner join (default) - only matching rows\n",
    "inner_merged = pd.merge(employees, departments, on='dept_id')\n",
    "print(\"Inner Join (employees + departments):\")\n",
    "print(inner_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join - all rows from left, matching from right\n",
    "left_merged = pd.merge(employees, salaries, on='emp_id', how='left')\n",
    "print(\"Left Join (employees + salaries):\")\n",
    "print(left_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Right join - all rows from right, matching from left\n",
    "right_merged = pd.merge(employees, salaries, on='emp_id', how='right')\n",
    "print(\"Right Join (employees + salaries):\")\n",
    "print(right_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer join - all rows from both\n",
    "outer_merged = pd.merge(employees, salaries, on='emp_id', how='outer')\n",
    "print(\"Outer Join (employees + salaries):\")\n",
    "print(outer_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on different column names\n",
    "df1 = pd.DataFrame({'id': [1, 2, 3], 'value': ['a', 'b', 'c']})\n",
    "df2 = pd.DataFrame({'key': [1, 2, 4], 'data': ['x', 'y', 'z']})\n",
    "\n",
    "merged = pd.merge(df1, df2, left_on='id', right_on='key', how='outer')\n",
    "print(\"Merge with different column names:\")\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple key columns\n",
    "sales_data = pd.DataFrame({\n",
    "    'year': [2020, 2020, 2021, 2021],\n",
    "    'quarter': ['Q1', 'Q2', 'Q1', 'Q2'],\n",
    "    'revenue': [100, 120, 110, 130]\n",
    "})\n",
    "\n",
    "targets = pd.DataFrame({\n",
    "    'year': [2020, 2020, 2021, 2021],\n",
    "    'quarter': ['Q1', 'Q2', 'Q1', 'Q2'],\n",
    "    'target': [95, 115, 105, 125]\n",
    "})\n",
    "\n",
    "merged = pd.merge(sales_data, targets, on=['year', 'quarter'])\n",
    "merged['achievement'] = (merged['revenue'] / merged['target'] * 100).round(1)\n",
    "print(\"Merge on multiple keys:\")\n",
    "print(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vertical concatenation (stacking rows)\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
    "\n",
    "print(\"df1:\")\n",
    "print(df1)\n",
    "print(\"\\ndf2:\")\n",
    "print(df2)\n",
    "\n",
    "vertical = pd.concat([df1, df2], ignore_index=True)\n",
    "print(\"\\nVertical concatenation:\")\n",
    "print(vertical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal concatenation (adding columns)\n",
    "df3 = pd.DataFrame({'C': [9, 10], 'D': [11, 12]})\n",
    "\n",
    "horizontal = pd.concat([df1, df3], axis=1)\n",
    "print(\"Horizontal concatenation:\")\n",
    "print(horizontal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenation with different columns\n",
    "df_a = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df_b = pd.DataFrame({'B': [5, 6], 'C': [7, 8]})\n",
    "\n",
    "# Outer join (default) - includes all columns\n",
    "outer_concat = pd.concat([df_a, df_b], ignore_index=True)\n",
    "print(\"Outer concat (all columns):\")\n",
    "print(outer_concat)\n",
    "\n",
    "# Inner join - only common columns\n",
    "inner_concat = pd.concat([df_a, df_b], join='inner', ignore_index=True)\n",
    "print(\"\\nInner concat (common columns only):\")\n",
    "print(inner_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Join (Index-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join uses index by default\n",
    "df1 = pd.DataFrame({'A': [1, 2, 3]}, index=['a', 'b', 'c'])\n",
    "df2 = pd.DataFrame({'B': [4, 5, 6]}, index=['a', 'b', 'd'])\n",
    "\n",
    "print(\"df1:\")\n",
    "print(df1)\n",
    "print(\"\\ndf2:\")\n",
    "print(df2)\n",
    "\n",
    "# Left join on index\n",
    "joined = df1.join(df2, how='outer')\n",
    "print(\"\\nJoin (outer):\")\n",
    "print(joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Assignment: Clean a Messy Dataset\n",
    "\n",
    "Let's create and clean a messy dataset with common data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messy dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "messy_data = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 2, 3, 4, 5, 5, 6, 7, 8, 9, 10, 11, 12, 13],\n",
    "    'name': ['John', 'Jane', 'Jane', 'Bob', 'Alice', 'Charlie', 'Charlie', \n",
    "             'Diana', None, 'Frank', 'Grace', 'Henry', 'Ivy', 'Jack', 'Kate'],\n",
    "    'age': [25, 30, 30, -5, 45, np.nan, np.nan, 28, 35, 150, 27, 33, np.nan, 29, 31],\n",
    "    'email': ['john@email.com', 'jane@email.com', 'jane@email.com', 'bob@email', \n",
    "              'alice@email.com', None, None, 'diana@email.com', 'eve@email.com',\n",
    "              'frank@email.com', 'grace@email.com', 'henry@email.com', 'ivy@email.com',\n",
    "              'jack@email.com', 'kate@email.com'],\n",
    "    'purchase_amount': [100, 200, 200, 150, np.nan, 300, 300, -50, 180, 220, 160, np.nan, 190, 210, 175],\n",
    "    'purchase_date': ['2023-01-15', '2023-02-20', '2023-02-20', '2023-03-10', '2023-04-05',\n",
    "                      '2023-05-12', '2023-05-12', '2023-06-18', '2023-07-22', '2023-08-30',\n",
    "                      '2023-09-14', '2023-10-25', '2023-11-08', '2023-12-01', '2024-01-05'],\n",
    "    'category': ['Electronics', 'Clothing', 'Clothing', 'electronics', 'CLOTHING', \n",
    "                 'Home', 'Home', 'Electronics', 'clothing', 'Home', 'Electronics',\n",
    "                 'Clothing', 'Home', 'Electronics', 'Clothing']\n",
    "})\n",
    "\n",
    "print(\"Messy Dataset:\")\n",
    "print(messy_data)\n",
    "print(f\"\\nShape: {messy_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify issues\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY ISSUES IDENTIFIED:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Duplicates\n",
    "print(f\"\\n1. Duplicate rows: {messy_data.duplicated().sum()}\")\n",
    "print(\"   Duplicate customer_ids:\", messy_data[messy_data.duplicated(subset=['customer_id'], keep=False)]['customer_id'].unique())\n",
    "\n",
    "# 2. Missing values\n",
    "print(f\"\\n2. Missing values:\")\n",
    "print(messy_data.isnull().sum()[messy_data.isnull().sum() > 0])\n",
    "\n",
    "# 3. Invalid ages\n",
    "print(f\"\\n3. Invalid ages (< 0 or > 120):\")\n",
    "invalid_ages = messy_data[(messy_data['age'] < 0) | (messy_data['age'] > 120)]\n",
    "print(invalid_ages[['customer_id', 'name', 'age']])\n",
    "\n",
    "# 4. Negative purchase amounts\n",
    "print(f\"\\n4. Negative purchase amounts:\")\n",
    "print(messy_data[messy_data['purchase_amount'] < 0][['customer_id', 'name', 'purchase_amount']])\n",
    "\n",
    "# 5. Inconsistent categories\n",
    "print(f\"\\n5. Inconsistent category values:\")\n",
    "print(messy_data['category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING STEP 1: Remove duplicates\n",
    "clean_df = messy_data.copy()\n",
    "rows_before = len(clean_df)\n",
    "\n",
    "clean_df = clean_df.drop_duplicates()\n",
    "print(f\"Step 1: Removed {rows_before - len(clean_df)} duplicate rows\")\n",
    "print(f\"        Rows remaining: {len(clean_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING STEP 2: Handle invalid ages\n",
    "# Replace invalid ages with NaN, then fill with median\n",
    "clean_df.loc[(clean_df['age'] < 0) | (clean_df['age'] > 120), 'age'] = np.nan\n",
    "median_age = clean_df['age'].median()\n",
    "clean_df['age'] = clean_df['age'].fillna(median_age)\n",
    "\n",
    "print(f\"Step 2: Replaced invalid ages with median ({median_age:.0f})\")\n",
    "print(f\"        Age range now: {clean_df['age'].min():.0f} - {clean_df['age'].max():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING STEP 3: Handle negative purchase amounts\n",
    "# Replace with absolute value (assuming data entry error)\n",
    "clean_df['purchase_amount'] = clean_df['purchase_amount'].abs()\n",
    "\n",
    "# Fill missing purchase amounts with median\n",
    "median_purchase = clean_df['purchase_amount'].median()\n",
    "clean_df['purchase_amount'] = clean_df['purchase_amount'].fillna(median_purchase)\n",
    "\n",
    "print(f\"Step 3: Fixed negative amounts and filled missing with median ({median_purchase:.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING STEP 4: Standardize categories\n",
    "clean_df['category'] = clean_df['category'].str.title()  # Capitalize first letter\n",
    "\n",
    "print(f\"Step 4: Standardized category values\")\n",
    "print(f\"        Unique categories: {clean_df['category'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING STEP 5: Handle missing names\n",
    "clean_df['name'] = clean_df['name'].fillna('Unknown')\n",
    "\n",
    "print(f\"Step 5: Filled missing names with 'Unknown'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING STEP 6: Convert purchase_date to datetime\n",
    "clean_df['purchase_date'] = pd.to_datetime(clean_df['purchase_date'])\n",
    "\n",
    "print(f\"Step 6: Converted purchase_date to datetime\")\n",
    "print(f\"        Date range: {clean_df['purchase_date'].min()} to {clean_df['purchase_date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURE ENGINEERING: Create new features\n",
    "clean_df['purchase_month'] = clean_df['purchase_date'].dt.month\n",
    "clean_df['purchase_quarter'] = clean_df['purchase_date'].dt.quarter\n",
    "clean_df['age_group'] = pd.cut(clean_df['age'], \n",
    "                                bins=[0, 25, 35, 50, 100], \n",
    "                                labels=['18-25', '26-35', '36-50', '50+'])\n",
    "\n",
    "print(\"Feature Engineering: Created new features\")\n",
    "print(f\"  - purchase_month\")\n",
    "print(f\"  - purchase_quarter\")\n",
    "print(f\"  - age_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleaned dataset\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLEANED DATASET:\")\n",
    "print(\"=\" * 60)\n",
    "print(clean_df)\n",
    "\n",
    "print(\"\\nData Types:\")\n",
    "print(clean_df.dtypes)\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(clean_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics of cleaned data\n",
    "print(\"\\nCleaned Data Summary:\")\n",
    "print(clean_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cleaned data\n",
    "print(\"\\nAnalysis of Cleaned Data:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nPurchase amount by category:\")\n",
    "print(clean_df.groupby('category')['purchase_amount'].agg(['count', 'mean', 'sum']).round(2))\n",
    "\n",
    "print(\"\\nPurchase amount by age group:\")\n",
    "print(clean_df.groupby('age_group')['purchase_amount'].agg(['count', 'mean', 'sum']).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cleaned data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Purchase by category\n",
    "category_sales = clean_df.groupby('category')['purchase_amount'].sum()\n",
    "axes[0, 0].bar(category_sales.index, category_sales.values, color='steelblue')\n",
    "axes[0, 0].set_title('Total Purchases by Category', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Total Amount')\n",
    "\n",
    "# Age distribution\n",
    "axes[0, 1].hist(clean_df['age'], bins=10, color='green', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('Age Distribution', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Age')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "\n",
    "# Purchase amount distribution\n",
    "axes[1, 0].hist(clean_df['purchase_amount'], bins=10, color='orange', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_title('Purchase Amount Distribution', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Amount')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "\n",
    "# Monthly trend\n",
    "monthly_sales = clean_df.groupby('purchase_month')['purchase_amount'].sum()\n",
    "axes[1, 1].plot(monthly_sales.index, monthly_sales.values, 'ro-', markersize=8, linewidth=2)\n",
    "axes[1, 1].set_title('Monthly Purchase Trend', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Month')\n",
    "axes[1, 1].set_ylabel('Total Amount')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Summary\n",
    "\n",
    "Today you learned:\n",
    "\n",
    "### DataFrames\n",
    "- Creation: `pd.DataFrame()` from dict, list, NumPy array\n",
    "- Exploration: `head()`, `tail()`, `info()`, `describe()`, `shape`\n",
    "- Indexing: `loc[]` (label), `iloc[]` (integer), `at[]`, `iat[]`\n",
    "- Filtering: Boolean indexing, `query()`, `isin()`, `between()`\n",
    "\n",
    "### Missing Data\n",
    "- Detection: `isnull()`, `sum()`\n",
    "- Removal: `dropna()` with `how`, `thresh`, `subset`\n",
    "- Filling: `fillna()`, `ffill()`, `bfill()`\n",
    "- Interpolation: `interpolate()` with different methods\n",
    "\n",
    "### GroupBy\n",
    "- Basic: `groupby()`, `agg()`, multiple aggregations\n",
    "- Transform: Create columns based on group statistics\n",
    "- Filter: Keep groups meeting certain criteria\n",
    "\n",
    "### Merging\n",
    "- `merge()`: inner, left, right, outer joins\n",
    "- `concat()`: vertical and horizontal concatenation\n",
    "- `join()`: index-based joining\n",
    "\n",
    "### Data Cleaning Process\n",
    "1. Identify issues (duplicates, missing, invalid values)\n",
    "2. Remove duplicates\n",
    "3. Handle invalid values\n",
    "4. Fill missing values appropriately\n",
    "5. Standardize categorical data\n",
    "6. Convert data types\n",
    "7. Create new features\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Tomorrow (Day 4), we'll dive into **Data Visualization** with Matplotlib and Seaborn!\n",
    "\n",
    "---\n",
    "\n",
    "**Great job completing Day 3!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
