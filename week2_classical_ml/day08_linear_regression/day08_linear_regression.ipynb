{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 8: Linear Regression\n",
    "\n",
    "## Week 2: Classical Machine Learning - Introduction\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Understand the mathematical foundations of linear regression\n",
    "2. Implement linear regression from scratch using NumPy\n",
    "3. Use Scikit-learn's LinearRegression model\n",
    "4. Evaluate models using MSE, RMSE, MAE, and R² score\n",
    "5. Build a house price prediction model\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction to Linear Regression](#1.-Introduction-to-Linear-Regression)\n",
    "2. [The Mathematics Behind Linear Regression](#2.-The-Mathematics-Behind-Linear-Regression)\n",
    "3. [Cost Function and Gradient Descent](#3.-Cost-Function-and-Gradient-Descent)\n",
    "4. [Implementation from Scratch](#4.-Implementation-from-Scratch)\n",
    "5. [Scikit-learn Implementation](#5.-Scikit-learn-Implementation)\n",
    "6. [Model Evaluation Metrics](#6.-Model-Evaluation-Metrics)\n",
    "7. [House Price Prediction Assignment](#7.-House-Price-Prediction-Assignment)\n",
    "8. [Comparison and Analysis](#8.-Comparison-and-Analysis)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Introduction to Linear Regression\n",
    "\n",
    "### What is Linear Regression?\n",
    "\n",
    "Linear Regression is a **supervised learning** algorithm used for predicting a **continuous** target variable based on one or more input features. It assumes a **linear relationship** between the input variables (X) and the output variable (y).\n",
    "\n",
    "### Types of Linear Regression\n",
    "\n",
    "1. **Simple Linear Regression**: One independent variable\n",
    "   - $y = mx + b$\n",
    "\n",
    "2. **Multiple Linear Regression**: Multiple independent variables\n",
    "   - $y = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- House price prediction\n",
    "- Sales forecasting\n",
    "- Stock price prediction\n",
    "- Medical outcome prediction\n",
    "- Customer lifetime value estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple example data to visualize linear regression\n",
    "np.random.seed(42)\n",
    "X_simple = np.linspace(0, 10, 50)\n",
    "y_simple = 2 * X_simple + 1 + np.random.normal(0, 1.5, 50)  # y = 2x + 1 + noise\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_simple, y_simple, color='blue', alpha=0.6, label='Data points')\n",
    "plt.plot(X_simple, 2 * X_simple + 1, color='red', linewidth=2, label='True relationship: y = 2x + 1')\n",
    "plt.xlabel('X (Feature)', fontsize=12)\n",
    "plt.ylabel('y (Target)', fontsize=12)\n",
    "plt.title('Simple Linear Regression Concept', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"The goal of linear regression is to find the best-fit line through the data points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. The Mathematics Behind Linear Regression\n",
    "\n",
    "### Simple Linear Regression: y = mx + b\n",
    "\n",
    "The equation of a line:\n",
    "\n",
    "$$\\hat{y} = mx + b$$\n",
    "\n",
    "Where:\n",
    "- $\\hat{y}$ = predicted value\n",
    "- $m$ = slope (coefficient) - how much y changes for each unit change in x\n",
    "- $x$ = input feature\n",
    "- $b$ = y-intercept (bias) - value of y when x = 0\n",
    "\n",
    "### Multiple Linear Regression\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n$$\n",
    "\n",
    "Or in vector notation:\n",
    "\n",
    "$$\\hat{y} = \\mathbf{X} \\cdot \\mathbf{w} + b$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{X}$ = feature matrix (n_samples x n_features)\n",
    "- $\\mathbf{w}$ = weight vector (coefficients)\n",
    "- $b$ = bias term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of slope and intercept\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "x_range = np.linspace(0, 5, 100)\n",
    "\n",
    "# Effect of changing slope (m)\n",
    "ax1 = axes[0]\n",
    "for m in [0.5, 1, 2, 3]:\n",
    "    ax1.plot(x_range, m * x_range + 1, label=f'm = {m}')\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_title('Effect of Slope (m) on Line\\n(b = 1 fixed)', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xlim(0, 5)\n",
    "ax1.set_ylim(0, 15)\n",
    "\n",
    "# Effect of changing intercept (b)\n",
    "ax2 = axes[1]\n",
    "for b in [-1, 0, 1, 2, 3]:\n",
    "    ax2.plot(x_range, 2 * x_range + b, label=f'b = {b}')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Effect of Intercept (b) on Line\\n(m = 2 fixed)', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(0, 5)\n",
    "ax2.set_ylim(-2, 15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analytical Solution: Ordinary Least Squares (OLS)\n",
    "\n",
    "For simple linear regression, the optimal parameters can be calculated directly:\n",
    "\n",
    "**Slope:**\n",
    "$$m = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^{n}(x_i - \\bar{x})^2} = \\frac{Cov(x, y)}{Var(x)}$$\n",
    "\n",
    "**Intercept:**\n",
    "$$b = \\bar{y} - m\\bar{x}$$\n",
    "\n",
    "For multiple linear regression, using matrix notation:\n",
    "\n",
    "$$\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "This is called the **Normal Equation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate parameters using OLS formulas\n",
    "def calculate_ols_parameters(X, y):\n",
    "    \"\"\"\n",
    "    Calculate slope and intercept using Ordinary Least Squares formulas\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Input features (1D array for simple linear regression)\n",
    "    y : array-like\n",
    "        Target values\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    m : float\n",
    "        Slope\n",
    "    b : float\n",
    "        Intercept\n",
    "    \"\"\"\n",
    "    # Calculate means\n",
    "    x_mean = np.mean(X)\n",
    "    y_mean = np.mean(y)\n",
    "    \n",
    "    # Calculate slope using formula: m = Cov(x,y) / Var(x)\n",
    "    numerator = np.sum((X - x_mean) * (y - y_mean))\n",
    "    denominator = np.sum((X - x_mean) ** 2)\n",
    "    m = numerator / denominator\n",
    "    \n",
    "    # Calculate intercept: b = y_mean - m * x_mean\n",
    "    b = y_mean - m * x_mean\n",
    "    \n",
    "    return m, b\n",
    "\n",
    "# Calculate for our simple example\n",
    "m_ols, b_ols = calculate_ols_parameters(X_simple, y_simple)\n",
    "\n",
    "print(\"OLS Solution for Simple Linear Regression\")\n",
    "print(\"=\"*50)\n",
    "print(f\"True parameters: m = 2.0, b = 1.0\")\n",
    "print(f\"Estimated parameters: m = {m_ols:.4f}, b = {b_ols:.4f}\")\n",
    "print(f\"\\nEquation: y = {m_ols:.4f}x + {b_ols:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the OLS fit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_simple, y_simple, color='blue', alpha=0.6, label='Data points')\n",
    "plt.plot(X_simple, m_ols * X_simple + b_ols, color='red', linewidth=2, \n",
    "         label=f'OLS fit: y = {m_ols:.2f}x + {b_ols:.2f}')\n",
    "plt.plot(X_simple, 2 * X_simple + 1, color='green', linewidth=2, linestyle='--', \n",
    "         label='True: y = 2x + 1')\n",
    "\n",
    "# Show residuals for a few points\n",
    "for i in range(0, len(X_simple), 10):\n",
    "    y_pred = m_ols * X_simple[i] + b_ols\n",
    "    plt.plot([X_simple[i], X_simple[i]], [y_simple[i], y_pred], \n",
    "             color='gray', linestyle=':', alpha=0.7)\n",
    "\n",
    "plt.xlabel('X (Feature)', fontsize=12)\n",
    "plt.ylabel('y (Target)', fontsize=12)\n",
    "plt.title('OLS Linear Regression Fit', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Cost Function and Gradient Descent\n",
    "\n",
    "### Cost Function (Loss Function)\n",
    "\n",
    "The cost function measures how well our model fits the data. For linear regression, we use **Mean Squared Error (MSE)**:\n",
    "\n",
    "$$J(m, b) = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - (mx_i + b))^2$$\n",
    "\n",
    "Our goal is to minimize this cost function.\n",
    "\n",
    "### Why MSE?\n",
    "\n",
    "1. **Differentiable**: Smooth surface for optimization\n",
    "2. **Convex**: Has a single global minimum\n",
    "3. **Penalizes larger errors more**: Squared term\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "Gradient descent is an iterative optimization algorithm that finds the minimum of the cost function:\n",
    "\n",
    "**Update Rules:**\n",
    "\n",
    "$$m = m - \\alpha \\frac{\\partial J}{\\partial m}$$\n",
    "$$b = b - \\alpha \\frac{\\partial J}{\\partial b}$$\n",
    "\n",
    "Where $\\alpha$ is the **learning rate**.\n",
    "\n",
    "**Gradients:**\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial m} = -\\frac{2}{n}\\sum_{i=1}^{n}x_i(y_i - \\hat{y}_i)$$\n",
    "$$\\frac{\\partial J}{\\partial b} = -\\frac{2}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the cost function surface\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "def compute_cost(X, y, m, b):\n",
    "    \"\"\"Compute MSE cost\"\"\"\n",
    "    n = len(y)\n",
    "    predictions = m * X + b\n",
    "    cost = (1/n) * np.sum((y - predictions) ** 2)\n",
    "    return cost\n",
    "\n",
    "# Create grid of m and b values\n",
    "m_range = np.linspace(0, 4, 50)\n",
    "b_range = np.linspace(-2, 4, 50)\n",
    "M, B = np.meshgrid(m_range, b_range)\n",
    "\n",
    "# Compute cost for each combination\n",
    "Z = np.zeros_like(M)\n",
    "for i in range(len(m_range)):\n",
    "    for j in range(len(b_range)):\n",
    "        Z[j, i] = compute_cost(X_simple, y_simple, m_range[i], b_range[j])\n",
    "\n",
    "# Plot cost surface\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 3D surface\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(M, B, Z, cmap='viridis', alpha=0.8)\n",
    "ax1.scatter([m_ols], [b_ols], [compute_cost(X_simple, y_simple, m_ols, b_ols)], \n",
    "            color='red', s=100, label='Minimum')\n",
    "ax1.set_xlabel('Slope (m)')\n",
    "ax1.set_ylabel('Intercept (b)')\n",
    "ax1.set_zlabel('Cost (MSE)')\n",
    "ax1.set_title('Cost Function Surface', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Contour plot\n",
    "ax2 = fig.add_subplot(122)\n",
    "contour = ax2.contour(M, B, Z, levels=20, cmap='viridis')\n",
    "ax2.clabel(contour, inline=True, fontsize=8)\n",
    "ax2.scatter([m_ols], [b_ols], color='red', s=100, zorder=5, label=f'Minimum: ({m_ols:.2f}, {b_ols:.2f})')\n",
    "ax2.set_xlabel('Slope (m)')\n",
    "ax2.set_ylabel('Intercept (b)')\n",
    "ax2.set_title('Cost Function Contour', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement gradient descent from scratch\n",
    "def gradient_descent(X, y, learning_rate=0.01, n_iterations=1000, verbose=False):\n",
    "    \"\"\"\n",
    "    Perform gradient descent to find optimal m and b\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Input features\n",
    "    y : array-like\n",
    "        Target values\n",
    "    learning_rate : float\n",
    "        Step size for parameter updates\n",
    "    n_iterations : int\n",
    "        Number of iterations\n",
    "    verbose : bool\n",
    "        Print progress\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    m, b : floats\n",
    "        Optimized parameters\n",
    "    history : dict\n",
    "        Training history\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    \n",
    "    # Initialize parameters randomly\n",
    "    m = np.random.randn()\n",
    "    b = np.random.randn()\n",
    "    \n",
    "    # Store history for visualization\n",
    "    history = {'m': [m], 'b': [b], 'cost': []}\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Predictions\n",
    "        y_pred = m * X + b\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = (1/n) * np.sum((y - y_pred) ** 2)\n",
    "        history['cost'].append(cost)\n",
    "        \n",
    "        # Compute gradients\n",
    "        dm = -(2/n) * np.sum(X * (y - y_pred))\n",
    "        db = -(2/n) * np.sum(y - y_pred)\n",
    "        \n",
    "        # Update parameters\n",
    "        m = m - learning_rate * dm\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # Store history\n",
    "        history['m'].append(m)\n",
    "        history['b'].append(b)\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and (i + 1) % (n_iterations // 10) == 0:\n",
    "            print(f\"Iteration {i+1}: Cost = {cost:.4f}, m = {m:.4f}, b = {b:.4f}\")\n",
    "    \n",
    "    return m, b, history\n",
    "\n",
    "# Run gradient descent\n",
    "print(\"Running Gradient Descent...\")\n",
    "print(\"=\"*60)\n",
    "m_gd, b_gd, history = gradient_descent(X_simple, y_simple, learning_rate=0.01, n_iterations=1000, verbose=True)\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"OLS Solution: m = {m_ols:.4f}, b = {b_ols:.4f}\")\n",
    "print(f\"GD Solution:  m = {m_gd:.4f}, b = {b_gd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gradient descent convergence\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Cost over iterations\n",
    "axes[0].plot(history['cost'], color='blue')\n",
    "axes[0].set_xlabel('Iteration')\n",
    "axes[0].set_ylabel('Cost (MSE)')\n",
    "axes[0].set_title('Cost Function Convergence', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Parameter trajectory on contour\n",
    "contour = axes[1].contour(M, B, Z, levels=20, cmap='viridis')\n",
    "axes[1].plot(history['m'], history['b'], 'r.-', alpha=0.5, markersize=2)\n",
    "axes[1].scatter(history['m'][0], history['b'][0], color='green', s=100, zorder=5, label='Start')\n",
    "axes[1].scatter(history['m'][-1], history['b'][-1], color='red', s=100, zorder=5, label='End')\n",
    "axes[1].set_xlabel('Slope (m)')\n",
    "axes[1].set_ylabel('Intercept (b)')\n",
    "axes[1].set_title('Gradient Descent Path', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "# Parameters over iterations\n",
    "axes[2].plot(history['m'], label='m (slope)', color='blue')\n",
    "axes[2].plot(history['b'], label='b (intercept)', color='orange')\n",
    "axes[2].axhline(y=m_ols, color='blue', linestyle='--', alpha=0.5, label=f'True m = {m_ols:.2f}')\n",
    "axes[2].axhline(y=b_ols, color='orange', linestyle='--', alpha=0.5, label=f'True b = {b_ols:.2f}')\n",
    "axes[2].set_xlabel('Iteration')\n",
    "axes[2].set_ylabel('Parameter Value')\n",
    "axes[2].set_title('Parameter Convergence', fontsize=12, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for i, lr in enumerate(learning_rates):\n",
    "    _, _, hist = gradient_descent(X_simple, y_simple, learning_rate=lr, n_iterations=500)\n",
    "    axes[i].plot(hist['cost'])\n",
    "    axes[i].set_xlabel('Iteration')\n",
    "    axes[i].set_ylabel('Cost (MSE)')\n",
    "    axes[i].set_title(f'Learning Rate = {lr}', fontsize=11, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Check for divergence\n",
    "    if hist['cost'][-1] > hist['cost'][0]:\n",
    "        axes[i].text(0.5, 0.5, 'DIVERGING!', transform=axes[i].transAxes, \n",
    "                    fontsize=14, color='red', ha='center')\n",
    "\n",
    "plt.suptitle('Effect of Learning Rate on Convergence', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Implementation from Scratch\n",
    "\n",
    "Let's implement a complete Linear Regression class from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionScratch:\n",
    "    \"\"\"\n",
    "    Linear Regression implemented from scratch using NumPy.\n",
    "    \n",
    "    Supports both OLS (analytical) and Gradient Descent optimization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000, method='ols'):\n",
    "        \"\"\"\n",
    "        Initialize Linear Regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        learning_rate : float\n",
    "            Learning rate for gradient descent\n",
    "        n_iterations : int\n",
    "            Number of iterations for gradient descent\n",
    "        method : str\n",
    "            'ols' for analytical solution, 'gd' for gradient descent\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.method = method\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the linear regression model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array-like of shape (n_samples,)\n",
    "            Target values\n",
    "        \"\"\"\n",
    "        # Ensure X is 2D\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        \n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        \n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        if self.method == 'ols':\n",
    "            self._fit_ols(X, y)\n",
    "        else:\n",
    "            self._fit_gradient_descent(X, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _fit_ols(self, X, y):\n",
    "        \"\"\"Fit using Normal Equation (OLS)\"\"\"\n",
    "        # Add bias column to X\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        # Normal equation: w = (X^T * X)^(-1) * X^T * y\n",
    "        theta = np.linalg.pinv(X_b.T @ X_b) @ X_b.T @ y\n",
    "        \n",
    "        self.bias = theta[0]\n",
    "        self.weights = theta[1:]\n",
    "        \n",
    "    def _fit_gradient_descent(self, X, y):\n",
    "        \"\"\"Fit using Gradient Descent\"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        self.cost_history = []\n",
    "        \n",
    "        for i in range(self.n_iterations):\n",
    "            # Predictions\n",
    "            y_pred = X @ self.weights + self.bias\n",
    "            \n",
    "            # Compute cost\n",
    "            cost = (1 / (2 * n_samples)) * np.sum((y - y_pred) ** 2)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = -(1 / n_samples) * X.T @ (y - y_pred)\n",
    "            db = -(1 / n_samples) * np.sum(y - y_pred)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Samples to predict\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : array of shape (n_samples,)\n",
    "            Predicted values\n",
    "        \"\"\"\n",
    "        X = np.array(X)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        return X @ self.weights + self.bias\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R² score.\n",
    "        \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        ss_res = np.sum((y - y_pred) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)\n",
    "\n",
    "\n",
    "# Test the implementation\n",
    "print(\"Testing Custom Linear Regression Implementation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare data\n",
    "X_train_simple = X_simple.reshape(-1, 1)\n",
    "\n",
    "# OLS method\n",
    "model_ols = LinearRegressionScratch(method='ols')\n",
    "model_ols.fit(X_train_simple, y_simple)\n",
    "print(f\"\\nOLS Method:\")\n",
    "print(f\"  Weights: {model_ols.weights}\")\n",
    "print(f\"  Bias: {model_ols.bias:.4f}\")\n",
    "print(f\"  R² Score: {model_ols.score(X_train_simple, y_simple):.4f}\")\n",
    "\n",
    "# Gradient Descent method\n",
    "model_gd = LinearRegressionScratch(method='gd', learning_rate=0.01, n_iterations=1000)\n",
    "model_gd.fit(X_train_simple, y_simple)\n",
    "print(f\"\\nGradient Descent Method:\")\n",
    "print(f\"  Weights: {model_gd.weights}\")\n",
    "print(f\"  Bias: {model_gd.bias:.4f}\")\n",
    "print(f\"  R² Score: {model_gd.score(X_train_simple, y_simple):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Scikit-learn Implementation\n",
    "\n",
    "Now let's use scikit-learn's LinearRegression for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn Linear Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create and fit model\n",
    "sklearn_model = LinearRegression()\n",
    "sklearn_model.fit(X_train_simple, y_simple)\n",
    "\n",
    "print(\"Scikit-learn Linear Regression\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Coefficients: {sklearn_model.coef_}\")\n",
    "print(f\"Intercept: {sklearn_model.intercept_:.4f}\")\n",
    "print(f\"R² Score: {sklearn_model.score(X_train_simple, y_simple):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three implementations\n",
    "print(\"\\nComparison of All Implementations\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Method':<25} {'Slope':<12} {'Intercept':<12} {'R² Score':<12}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Manual OLS':<25} {m_ols:<12.4f} {b_ols:<12.4f} {model_ols.score(X_train_simple, y_simple):<12.4f}\")\n",
    "print(f\"{'Custom Class (OLS)':<25} {model_ols.weights[0]:<12.4f} {model_ols.bias:<12.4f} {model_ols.score(X_train_simple, y_simple):<12.4f}\")\n",
    "print(f\"{'Custom Class (GD)':<25} {model_gd.weights[0]:<12.4f} {model_gd.bias:<12.4f} {model_gd.score(X_train_simple, y_simple):<12.4f}\")\n",
    "print(f\"{'Scikit-learn':<25} {sklearn_model.coef_[0]:<12.4f} {sklearn_model.intercept_:<12.4f} {sklearn_model.score(X_train_simple, y_simple):<12.4f}\")\n",
    "print(f\"{'True Values':<25} {2.0:<12.4f} {1.0:<12.4f} {'N/A':<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions from all methods\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.scatter(X_simple, y_simple, color='blue', alpha=0.5, label='Data')\n",
    "plt.plot(X_simple, model_ols.predict(X_train_simple), 'r-', linewidth=2, label='Custom OLS')\n",
    "plt.plot(X_simple, model_gd.predict(X_train_simple), 'g--', linewidth=2, label='Custom GD')\n",
    "plt.plot(X_simple, sklearn_model.predict(X_train_simple), 'm:', linewidth=3, label='Scikit-learn')\n",
    "\n",
    "plt.xlabel('X', fontsize=12)\n",
    "plt.ylabel('y', fontsize=12)\n",
    "plt.title('Comparison of Linear Regression Implementations', fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Evaluation Metrics\n",
    "\n",
    "### Key Metrics for Regression\n",
    "\n",
    "1. **Mean Squared Error (MSE)**\n",
    "   $$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "2. **Root Mean Squared Error (RMSE)**\n",
    "   $$RMSE = \\sqrt{MSE}$$\n",
    "\n",
    "3. **Mean Absolute Error (MAE)**\n",
    "   $$MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i|$$\n",
    "\n",
    "4. **R² Score (Coefficient of Determination)**\n",
    "   $$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement evaluation metrics from scratch\n",
    "def mean_squared_error_manual(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Squared Error\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def root_mean_squared_error_manual(y_true, y_pred):\n",
    "    \"\"\"Calculate Root Mean Squared Error\"\"\"\n",
    "    return np.sqrt(mean_squared_error_manual(y_true, y_pred))\n",
    "\n",
    "def mean_absolute_error_manual(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Error\"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def r2_score_manual(y_true, y_pred):\n",
    "    \"\"\"Calculate R² Score\"\"\"\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Calculate metrics\n",
    "y_pred = sklearn_model.predict(X_train_simple)\n",
    "\n",
    "print(\"Model Evaluation Metrics\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Metric':<30} {'Manual':<15} {'Sklearn':<15}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'MSE':<30} {mean_squared_error_manual(y_simple, y_pred):<15.4f} {mean_squared_error(y_simple, y_pred):<15.4f}\")\n",
    "print(f\"{'RMSE':<30} {root_mean_squared_error_manual(y_simple, y_pred):<15.4f} {np.sqrt(mean_squared_error(y_simple, y_pred)):<15.4f}\")\n",
    "print(f\"{'MAE':<30} {mean_absolute_error_manual(y_simple, y_pred):<15.4f} {mean_absolute_error(y_simple, y_pred):<15.4f}\")\n",
    "print(f\"{'R² Score':<30} {r2_score_manual(y_simple, y_pred):<15.4f} {r2_score(y_simple, y_pred):<15.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding R² Score\n",
    "print(\"\\nUnderstanding R² Score\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "R² Score Interpretation:\n",
    "- R² = 1.0: Perfect predictions (all variance explained)\n",
    "- R² = 0.0: Model predicts mean value only (no variance explained)\n",
    "- R² < 0.0: Model is worse than predicting mean (possible overfitting)\n",
    "\n",
    "General Guidelines:\n",
    "- R² > 0.9: Excellent fit\n",
    "- 0.7 < R² < 0.9: Good fit\n",
    "- 0.5 < R² < 0.7: Moderate fit\n",
    "- R² < 0.5: Weak fit\n",
    "\"\"\")\n",
    "\n",
    "# Visualize R² concept\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# High R²\n",
    "X_demo = np.linspace(0, 10, 50)\n",
    "y_high_r2 = 2 * X_demo + 1 + np.random.normal(0, 0.5, 50)\n",
    "model_temp = LinearRegression().fit(X_demo.reshape(-1, 1), y_high_r2)\n",
    "axes[0].scatter(X_demo, y_high_r2, alpha=0.6)\n",
    "axes[0].plot(X_demo, model_temp.predict(X_demo.reshape(-1, 1)), 'r-', linewidth=2)\n",
    "axes[0].set_title(f'High R² = {model_temp.score(X_demo.reshape(-1,1), y_high_r2):.3f}', fontweight='bold')\n",
    "\n",
    "# Medium R²\n",
    "y_med_r2 = 2 * X_demo + 1 + np.random.normal(0, 3, 50)\n",
    "model_temp = LinearRegression().fit(X_demo.reshape(-1, 1), y_med_r2)\n",
    "axes[1].scatter(X_demo, y_med_r2, alpha=0.6)\n",
    "axes[1].plot(X_demo, model_temp.predict(X_demo.reshape(-1, 1)), 'r-', linewidth=2)\n",
    "axes[1].set_title(f'Medium R² = {model_temp.score(X_demo.reshape(-1,1), y_med_r2):.3f}', fontweight='bold')\n",
    "\n",
    "# Low R²\n",
    "y_low_r2 = 2 * X_demo + 1 + np.random.normal(0, 8, 50)\n",
    "model_temp = LinearRegression().fit(X_demo.reshape(-1, 1), y_low_r2)\n",
    "axes[2].scatter(X_demo, y_low_r2, alpha=0.6)\n",
    "axes[2].plot(X_demo, model_temp.predict(X_demo.reshape(-1, 1)), 'r-', linewidth=2)\n",
    "axes[2].set_title(f'Low R² = {model_temp.score(X_demo.reshape(-1,1), y_low_r2):.3f}', fontweight='bold')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Understanding R² Score', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. House Price Prediction Assignment\n",
    "\n",
    "### Task: Predict house prices using linear regression\n",
    "\n",
    "We'll create a realistic house price dataset and build prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create House Price Dataset\n",
    "np.random.seed(42)\n",
    "n_houses = 1000\n",
    "\n",
    "# Generate features\n",
    "house_data = pd.DataFrame({\n",
    "    'sqft': np.random.normal(1800, 500, n_houses).clip(500, 5000),\n",
    "    'bedrooms': np.random.choice([1, 2, 3, 4, 5, 6], n_houses, p=[0.05, 0.15, 0.35, 0.30, 0.10, 0.05]),\n",
    "    'bathrooms': np.random.choice([1, 1.5, 2, 2.5, 3, 3.5, 4], n_houses, p=[0.10, 0.15, 0.25, 0.20, 0.15, 0.10, 0.05]),\n",
    "    'age': np.random.randint(0, 100, n_houses),\n",
    "    'lot_size': np.random.normal(8000, 3000, n_houses).clip(1000, 30000),\n",
    "    'garage_cars': np.random.choice([0, 1, 2, 3], n_houses, p=[0.10, 0.30, 0.45, 0.15]),\n",
    "    'has_pool': np.random.choice([0, 1], n_houses, p=[0.75, 0.25]),\n",
    "    'neighborhood_quality': np.random.choice([1, 2, 3, 4, 5], n_houses, p=[0.05, 0.15, 0.40, 0.30, 0.10])\n",
    "})\n",
    "\n",
    "# Generate price based on features (with realistic coefficients)\n",
    "house_data['price'] = (\n",
    "    50000 +                                        # base price\n",
    "    150 * house_data['sqft'] +                     # $150 per sqft\n",
    "    15000 * house_data['bedrooms'] +               # $15k per bedroom\n",
    "    20000 * house_data['bathrooms'] +              # $20k per bathroom\n",
    "    -1000 * house_data['age'] +                    # -$1k per year of age\n",
    "    5 * house_data['lot_size'] +                   # $5 per sqft of lot\n",
    "    25000 * house_data['garage_cars'] +            # $25k per garage space\n",
    "    40000 * house_data['has_pool'] +               # $40k for pool\n",
    "    50000 * house_data['neighborhood_quality'] +   # $50k per quality level\n",
    "    np.random.normal(0, 30000, n_houses)           # noise\n",
    ").clip(50000, 2000000)\n",
    "\n",
    "print(\"House Price Dataset Created\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {house_data.shape}\")\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "house_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDA for House Data\n",
    "print(\"Dataset Statistics\")\n",
    "print(\"=\"*60)\n",
    "print(house_data.describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation = house_data.corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='RdYlBu_r', center=0, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCorrelations with Price:\")\n",
    "price_corr = correlation['price'].drop('price').sort_values(ascending=False)\n",
    "for feat, corr in price_corr.items():\n",
    "    print(f\"  {feat}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots of top features vs price\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "features = ['sqft', 'bedrooms', 'bathrooms', 'age', 'lot_size', 'garage_cars', 'has_pool', 'neighborhood_quality']\n",
    "\n",
    "for i, feat in enumerate(features):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    ax.scatter(house_data[feat], house_data['price']/1000, alpha=0.3)\n",
    "    ax.set_xlabel(feat)\n",
    "    ax.set_ylabel('Price ($K)')\n",
    "    ax.set_title(f'Price vs {feat}', fontsize=10, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Feature Relationships with House Price', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X_house = house_data.drop('price', axis=1)\n",
    "y_house = house_data['price']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_house, y_house, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Data Split\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled using StandardScaler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Manual Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual implementation with gradient descent\n",
    "manual_model = LinearRegressionScratch(method='gd', learning_rate=0.1, n_iterations=2000)\n",
    "manual_model.fit(X_train_scaled, y_train.values)\n",
    "\n",
    "# Predictions\n",
    "y_pred_manual_train = manual_model.predict(X_train_scaled)\n",
    "y_pred_manual_test = manual_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Manual Implementation (Gradient Descent)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining Metrics:\")\n",
    "print(f\"  RMSE: ${np.sqrt(mean_squared_error(y_train, y_pred_manual_train)):,.2f}\")\n",
    "print(f\"  MAE: ${mean_absolute_error(y_train, y_pred_manual_train):,.2f}\")\n",
    "print(f\"  R²: {r2_score(y_train, y_pred_manual_train):.4f}\")\n",
    "\n",
    "print(f\"\\nTest Metrics:\")\n",
    "print(f\"  RMSE: ${np.sqrt(mean_squared_error(y_test, y_pred_manual_test)):,.2f}\")\n",
    "print(f\"  MAE: ${mean_absolute_error(y_test, y_pred_manual_test):,.2f}\")\n",
    "print(f\"  R²: {r2_score(y_test, y_pred_manual_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot cost convergence\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(manual_model.cost_history)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost (MSE/2)')\n",
    "plt.title('Gradient Descent Convergence - House Price Model', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Scikit-learn Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn implementation\n",
    "sklearn_house_model = LinearRegression()\n",
    "sklearn_house_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_sklearn_train = sklearn_house_model.predict(X_train_scaled)\n",
    "y_pred_sklearn_test = sklearn_house_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Scikit-learn Implementation\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTraining Metrics:\")\n",
    "print(f\"  RMSE: ${np.sqrt(mean_squared_error(y_train, y_pred_sklearn_train)):,.2f}\")\n",
    "print(f\"  MAE: ${mean_absolute_error(y_train, y_pred_sklearn_train):,.2f}\")\n",
    "print(f\"  R²: {r2_score(y_train, y_pred_sklearn_train):.4f}\")\n",
    "\n",
    "print(f\"\\nTest Metrics:\")\n",
    "print(f\"  RMSE: ${np.sqrt(mean_squared_error(y_test, y_pred_sklearn_test)):,.2f}\")\n",
    "print(f\"  MAE: ${mean_absolute_error(y_test, y_pred_sklearn_test):,.2f}\")\n",
    "print(f\"  R²: {r2_score(y_test, y_pred_sklearn_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (coefficients)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_house.columns,\n",
    "    'Coefficient_Manual': manual_model.weights,\n",
    "    'Coefficient_Sklearn': sklearn_house_model.coef_\n",
    "}).sort_values('Coefficient_Sklearn', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nFeature Coefficients (Scaled Data):\")\n",
    "print(feature_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "feature_importance_sorted = feature_importance.sort_values('Coefficient_Sklearn')\n",
    "colors = ['green' if x > 0 else 'red' for x in feature_importance_sorted['Coefficient_Sklearn']]\n",
    "\n",
    "ax.barh(feature_importance_sorted['Feature'], feature_importance_sorted['Coefficient_Sklearn'], color=colors)\n",
    "ax.axvline(x=0, color='black', linewidth=0.5)\n",
    "ax.set_xlabel('Coefficient Value (Effect on Price)')\n",
    "ax.set_title('Feature Importance - House Price Prediction', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Comparison and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison\n",
    "print(\"FINAL MODEL COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Manual (GD)', 'Scikit-learn'],\n",
    "    'Train RMSE': [\n",
    "        np.sqrt(mean_squared_error(y_train, y_pred_manual_train)),\n",
    "        np.sqrt(mean_squared_error(y_train, y_pred_sklearn_train))\n",
    "    ],\n",
    "    'Test RMSE': [\n",
    "        np.sqrt(mean_squared_error(y_test, y_pred_manual_test)),\n",
    "        np.sqrt(mean_squared_error(y_test, y_pred_sklearn_test))\n",
    "    ],\n",
    "    'Train R²': [\n",
    "        r2_score(y_train, y_pred_manual_train),\n",
    "        r2_score(y_train, y_pred_sklearn_train)\n",
    "    ],\n",
    "    'Test R²': [\n",
    "        r2_score(y_test, y_pred_manual_test),\n",
    "        r2_score(y_test, y_pred_sklearn_test)\n",
    "    ]\n",
    "})\n",
    "\n",
    "results['Train RMSE'] = results['Train RMSE'].apply(lambda x: f\"${x:,.0f}\")\n",
    "results['Test RMSE'] = results['Test RMSE'].apply(lambda x: f\"${x:,.0f}\")\n",
    "results['Train R²'] = results['Train R²'].apply(lambda x: f\"{x:.4f}\")\n",
    "results['Test R²'] = results['Test R²'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction vs Actual plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Manual model\n",
    "axes[0].scatter(y_test, y_pred_manual_test, alpha=0.5)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0].set_xlabel('Actual Price ($)')\n",
    "axes[0].set_ylabel('Predicted Price ($)')\n",
    "axes[0].set_title(f'Manual Model: Predicted vs Actual\\nR² = {r2_score(y_test, y_pred_manual_test):.4f}', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sklearn model\n",
    "axes[1].scatter(y_test, y_pred_sklearn_test, alpha=0.5)\n",
    "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "axes[1].set_xlabel('Actual Price ($)')\n",
    "axes[1].set_ylabel('Predicted Price ($)')\n",
    "axes[1].set_title(f'Sklearn Model: Predicted vs Actual\\nR² = {r2_score(y_test, y_pred_sklearn_test):.4f}', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "residuals_sklearn = y_test - y_pred_sklearn_test\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Residual distribution\n",
    "axes[0].hist(residuals_sklearn, bins=30, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[0].set_xlabel('Residual ($)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Residual Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Residuals vs Predicted\n",
    "axes[1].scatter(y_pred_sklearn_test, residuals_sklearn, alpha=0.5)\n",
    "axes[1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1].set_xlabel('Predicted Price ($)')\n",
    "axes[1].set_ylabel('Residual ($)')\n",
    "axes[1].set_title('Residuals vs Predicted', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot\n",
    "stats.probplot(residuals_sklearn, dist=\"norm\", plot=axes[2])\n",
    "axes[2].set_title('Q-Q Plot (Normality Check)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Residual Analysis', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation\n",
    "print(\"\\nCross-Validation Results (5-Fold)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cv_scores = cross_val_score(sklearn_house_model, X_train_scaled, y_train, cv=5, scoring='r2')\n",
    "cv_rmse = cross_val_score(sklearn_house_model, X_train_scaled, y_train, cv=5, \n",
    "                          scoring='neg_root_mean_squared_error')\n",
    "\n",
    "print(f\"\\nR² Scores: {cv_scores.round(4)}\")\n",
    "print(f\"Mean R²: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f})\")\n",
    "print(f\"\\nRMSE: ${-cv_rmse.mean():,.2f} (+/- ${cv_rmse.std()*2:,.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Linear Regression Basics**\n",
    "   - Simple equation: y = mx + b\n",
    "   - Multiple regression: y = w₀ + w₁x₁ + w₂x₂ + ...\n",
    "   - Assumes linear relationship between features and target\n",
    "\n",
    "2. **Cost Function and Optimization**\n",
    "   - MSE measures prediction error\n",
    "   - Gradient descent iteratively minimizes cost\n",
    "   - Learning rate controls convergence speed\n",
    "   - OLS provides analytical solution (faster but less flexible)\n",
    "\n",
    "3. **Evaluation Metrics**\n",
    "   - MSE/RMSE: Average squared/root error\n",
    "   - MAE: Average absolute error\n",
    "   - R²: Proportion of variance explained (0 to 1)\n",
    "\n",
    "4. **Best Practices**\n",
    "   - Scale features before gradient descent\n",
    "   - Use train-test split for evaluation\n",
    "   - Check residual plots for model assumptions\n",
    "   - Use cross-validation for robust assessment\n",
    "\n",
    "### When to Use Linear Regression\n",
    "\n",
    "**Use When:**\n",
    "- Target variable is continuous\n",
    "- Relationship appears linear\n",
    "- Need interpretable model\n",
    "- Quick baseline model\n",
    "\n",
    "**Avoid When:**\n",
    "- Non-linear relationships\n",
    "- Target is categorical\n",
    "- High multicollinearity\n",
    "- Complex feature interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for future use\n",
    "import pickle\n",
    "\n",
    "# Save sklearn model\n",
    "with open('house_price_model.pkl', 'wb') as f:\n",
    "    pickle.dump(sklearn_house_model, f)\n",
    "\n",
    "# Save scaler\n",
    "with open('house_price_scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "print(\"Model and scaler saved!\")\n",
    "print(\"Files: house_price_model.pkl, house_price_scaler.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
