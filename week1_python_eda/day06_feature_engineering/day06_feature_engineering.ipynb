{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6: Feature Engineering Part 1\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you will be able to:\n",
    "1. Understand and apply feature scaling techniques (standardization vs normalization)\n",
    "2. Encode categorical variables using one-hot encoding and label encoding\n",
    "3. Create new features from existing data\n",
    "4. Handle datetime features effectively\n",
    "5. Transform raw datasets into ML-ready format\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. [Introduction to Feature Engineering](#1.-Introduction-to-Feature-Engineering)\n",
    "2. [Feature Scaling](#2.-Feature-Scaling)\n",
    "   - 2.1 Why Scale Features?\n",
    "   - 2.2 Standardization (Z-score Normalization)\n",
    "   - 2.3 Min-Max Normalization\n",
    "   - 2.4 Robust Scaling\n",
    "   - 2.5 When to Use Which?\n",
    "3. [Encoding Categorical Variables](#3.-Encoding-Categorical-Variables)\n",
    "   - 3.1 Label Encoding\n",
    "   - 3.2 One-Hot Encoding\n",
    "   - 3.3 Ordinal Encoding\n",
    "   - 3.4 Target Encoding\n",
    "4. [Feature Creation](#4.-Feature-Creation)\n",
    "   - 4.1 Polynomial Features\n",
    "   - 4.2 Interaction Features\n",
    "   - 4.3 Aggregation Features\n",
    "   - 4.4 Domain-Specific Features\n",
    "5. [Handling DateTime Features](#5.-Handling-DateTime-Features)\n",
    "6. [Comprehensive Assignment](#6.-Assignment:-Transform-Raw-Dataset)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler,\n",
    "    MinMaxScaler,\n",
    "    RobustScaler,\n",
    "    LabelEncoder,\n",
    "    OneHotEncoder,\n",
    "    OrdinalEncoder,\n",
    "    PolynomialFeatures\n",
    ")\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Introduction to Feature Engineering\n",
    "\n",
    "**Feature Engineering** is the process of transforming raw data into features that better represent the underlying problem to predictive models, resulting in improved model accuracy on unseen data.\n",
    "\n",
    "### Why is Feature Engineering Important?\n",
    "\n",
    "1. **Better Data Representation**: Raw data often needs transformation to be useful for ML algorithms\n",
    "2. **Improved Model Performance**: Good features can significantly boost model accuracy\n",
    "3. **Algorithm Requirements**: Many algorithms require numerical inputs and specific data formats\n",
    "4. **Domain Knowledge Integration**: Allows incorporating expert knowledge into the data\n",
    "\n",
    "### The Feature Engineering Process\n",
    "\n",
    "```\n",
    "Raw Data → Data Cleaning → Feature Transformation → Feature Creation → Feature Selection → ML-Ready Data\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset for demonstrations\n",
    "np.random.seed(42)\n",
    "\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate sample data\n",
    "sample_data = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 80, n_samples),\n",
    "    'income': np.random.exponential(50000, n_samples) + 20000,\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], n_samples, \n",
    "                                   p=[0.3, 0.4, 0.2, 0.1]),\n",
    "    'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_samples),\n",
    "    'purchase_date': pd.date_range('2023-01-01', periods=n_samples, freq='8H'),\n",
    "    'transaction_amount': np.random.exponential(100, n_samples) + 10,\n",
    "    'rating': np.random.choice(['Poor', 'Average', 'Good', 'Excellent'], n_samples,\n",
    "                               p=[0.1, 0.3, 0.4, 0.2])\n",
    "})\n",
    "\n",
    "print(\"Sample Dataset Shape:\", sample_data.shape)\n",
    "print(\"\\nColumn Types:\")\n",
    "print(sample_data.dtypes)\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "sample_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Feature Scaling\n",
    "\n",
    "Feature scaling is crucial for many machine learning algorithms. Different features often have different scales, which can cause problems during model training.\n",
    "\n",
    "### 2.1 Why Scale Features?\n",
    "\n",
    "Many ML algorithms are sensitive to the scale of features:\n",
    "\n",
    "| Algorithm | Needs Scaling? | Reason |\n",
    "|-----------|---------------|--------|\n",
    "| Linear/Logistic Regression | Yes | Gradient descent converges faster |\n",
    "| SVM | Yes | Distance-based algorithm |\n",
    "| K-Nearest Neighbors | Yes | Distance-based algorithm |\n",
    "| Neural Networks | Yes | Gradient-based optimization |\n",
    "| PCA | Yes | Variance-based algorithm |\n",
    "| Decision Trees | No | Split-based, scale-invariant |\n",
    "| Random Forest | No | Ensemble of trees |\n",
    "| XGBoost/LightGBM | No | Tree-based |\n",
    "\n",
    "### The Problem with Unscaled Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the problem with unscaled features\n",
    "print(\"Statistics of Unscaled Features:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nAge:\")\n",
    "print(f\"  Range: {sample_data['age'].min()} - {sample_data['age'].max()}\")\n",
    "print(f\"  Mean: {sample_data['age'].mean():.2f}\")\n",
    "print(f\"  Std: {sample_data['age'].std():.2f}\")\n",
    "\n",
    "print(f\"\\nIncome:\")\n",
    "print(f\"  Range: ${sample_data['income'].min():,.2f} - ${sample_data['income'].max():,.2f}\")\n",
    "print(f\"  Mean: ${sample_data['income'].mean():,.2f}\")\n",
    "print(f\"  Std: ${sample_data['income'].std():,.2f}\")\n",
    "\n",
    "print(f\"\\nTransaction Amount:\")\n",
    "print(f\"  Range: ${sample_data['transaction_amount'].min():.2f} - ${sample_data['transaction_amount'].max():.2f}\")\n",
    "print(f\"  Mean: ${sample_data['transaction_amount'].mean():.2f}\")\n",
    "print(f\"  Std: ${sample_data['transaction_amount'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the scale difference\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot distributions\n",
    "axes[0].hist(sample_data['age'], bins=30, color='skyblue', edgecolor='black')\n",
    "axes[0].set_title('Age Distribution', fontsize=12)\n",
    "axes[0].set_xlabel('Age')\n",
    "\n",
    "axes[1].hist(sample_data['income'], bins=30, color='lightgreen', edgecolor='black')\n",
    "axes[1].set_title('Income Distribution', fontsize=12)\n",
    "axes[1].set_xlabel('Income ($)')\n",
    "\n",
    "axes[2].hist(sample_data['transaction_amount'], bins=30, color='salmon', edgecolor='black')\n",
    "axes[2].set_title('Transaction Amount Distribution', fontsize=12)\n",
    "axes[2].set_xlabel('Amount ($)')\n",
    "\n",
    "plt.suptitle('Original Feature Distributions (Different Scales)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Standardization (Z-score Normalization)\n",
    "\n",
    "Standardization transforms features to have **zero mean** and **unit variance**.\n",
    "\n",
    "**Formula:**\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "- $x$ = original value\n",
    "- $\\mu$ = mean of the feature\n",
    "- $\\sigma$ = standard deviation of the feature\n",
    "\n",
    "**Properties:**\n",
    "- Resulting values are typically between -3 and 3\n",
    "- Does NOT bound values to a specific range\n",
    "- Less affected by outliers than Min-Max\n",
    "- Best for normally distributed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Standardization Implementation\n",
    "def standardize_manual(X):\n",
    "    \"\"\"\n",
    "    Manually standardize features (Z-score normalization)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Input features to standardize\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_standardized : array\n",
    "        Standardized features with mean=0 and std=1\n",
    "    \"\"\"\n",
    "    mean = np.mean(X)\n",
    "    std = np.std(X)\n",
    "    X_standardized = (X - mean) / std\n",
    "    return X_standardized, mean, std\n",
    "\n",
    "# Apply manual standardization\n",
    "income_standardized_manual, income_mean, income_std = standardize_manual(sample_data['income'])\n",
    "\n",
    "print(\"Manual Standardization:\")\n",
    "print(f\"Original Mean: ${income_mean:,.2f}\")\n",
    "print(f\"Original Std: ${income_std:,.2f}\")\n",
    "print(f\"Standardized Mean: {np.mean(income_standardized_manual):.10f} (should be ~0)\")\n",
    "print(f\"Standardized Std: {np.std(income_standardized_manual):.10f} (should be ~1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Scikit-learn's StandardScaler\n",
    "numerical_features = ['age', 'income', 'transaction_amount']\n",
    "\n",
    "# Initialize the scaler\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform\n",
    "data_standardized = standard_scaler.fit_transform(sample_data[numerical_features])\n",
    "df_standardized = pd.DataFrame(data_standardized, columns=[f'{col}_standardized' for col in numerical_features])\n",
    "\n",
    "print(\"Scikit-learn StandardScaler Results:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nStatistics after standardization:\")\n",
    "print(df_standardized.describe().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize standardized features\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, col in enumerate(numerical_features):\n",
    "    axes[i].hist(df_standardized[f'{col}_standardized'], bins=30, color='steelblue', edgecolor='black')\n",
    "    axes[i].axvline(x=0, color='red', linestyle='--', label='Mean (0)')\n",
    "    axes[i].set_title(f'{col.title()} (Standardized)', fontsize=12)\n",
    "    axes[i].set_xlabel('Standardized Value')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle('Standardized Feature Distributions (Same Scale)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Min-Max Normalization\n",
    "\n",
    "Min-Max Normalization scales features to a fixed range, typically [0, 1].\n",
    "\n",
    "**Formula:**\n",
    "$$x_{normalized} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "**For custom range [a, b]:**\n",
    "$$x_{scaled} = a + \\frac{(x - x_{min})(b - a)}{x_{max} - x_{min}}$$\n",
    "\n",
    "**Properties:**\n",
    "- Bounds values to [0, 1] (or specified range)\n",
    "- Preserves zero entries in sparse data\n",
    "- Sensitive to outliers (they compress the majority of data)\n",
    "- Good for image data (pixel values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Min-Max Normalization Implementation\n",
    "def normalize_minmax_manual(X, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    Manually normalize features using Min-Max scaling\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Input features to normalize\n",
    "    feature_range : tuple\n",
    "        Desired range for the normalized data (default: (0, 1))\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X_normalized : array\n",
    "        Normalized features in specified range\n",
    "    \"\"\"\n",
    "    X_min = np.min(X)\n",
    "    X_max = np.max(X)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    X_normalized = (X - X_min) / (X_max - X_min)\n",
    "    \n",
    "    # Scale to desired range\n",
    "    range_min, range_max = feature_range\n",
    "    X_normalized = X_normalized * (range_max - range_min) + range_min\n",
    "    \n",
    "    return X_normalized, X_min, X_max\n",
    "\n",
    "# Apply manual normalization\n",
    "income_normalized_manual, _, _ = normalize_minmax_manual(sample_data['income'])\n",
    "\n",
    "print(\"Manual Min-Max Normalization:\")\n",
    "print(f\"Min: {np.min(income_normalized_manual):.4f} (should be 0)\")\n",
    "print(f\"Max: {np.max(income_normalized_manual):.4f} (should be 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Scikit-learn's MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit and transform\n",
    "data_normalized = minmax_scaler.fit_transform(sample_data[numerical_features])\n",
    "df_normalized = pd.DataFrame(data_normalized, columns=[f'{col}_normalized' for col in numerical_features])\n",
    "\n",
    "print(\"Scikit-learn MinMaxScaler Results:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nStatistics after normalization:\")\n",
    "print(df_normalized.describe().round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize normalized features\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, col in enumerate(numerical_features):\n",
    "    axes[i].hist(df_normalized[f'{col}_normalized'], bins=30, color='coral', edgecolor='black')\n",
    "    axes[i].axvline(x=0, color='blue', linestyle='--', label='Min (0)')\n",
    "    axes[i].axvline(x=1, color='green', linestyle='--', label='Max (1)')\n",
    "    axes[i].set_title(f'{col.title()} (Normalized)', fontsize=12)\n",
    "    axes[i].set_xlabel('Normalized Value [0, 1]')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle('Min-Max Normalized Feature Distributions', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Robust Scaling\n",
    "\n",
    "Robust Scaling uses statistics that are robust to outliers: **median** and **interquartile range (IQR)**.\n",
    "\n",
    "**Formula:**\n",
    "$$x_{robust} = \\frac{x - Q_2}{Q_3 - Q_1}$$\n",
    "\n",
    "Where:\n",
    "- $Q_1$ = 25th percentile (first quartile)\n",
    "- $Q_2$ = 50th percentile (median)\n",
    "- $Q_3$ = 75th percentile (third quartile)\n",
    "\n",
    "**Properties:**\n",
    "- Not affected by outliers\n",
    "- Centers around the median\n",
    "- Spreads based on IQR\n",
    "- Best for data with many outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with outliers\n",
    "np.random.seed(42)\n",
    "data_with_outliers = np.concatenate([\n",
    "    np.random.normal(50, 10, 950),  # Normal data\n",
    "    np.random.normal(200, 20, 50)    # Outliers\n",
    "])\n",
    "\n",
    "# Compare scalers on data with outliers\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Original data\n",
    "axes[0, 0].hist(data_with_outliers, bins=50, color='gray', edgecolor='black')\n",
    "axes[0, 0].set_title('Original Data (with outliers)', fontsize=12)\n",
    "axes[0, 0].axvline(x=np.mean(data_with_outliers), color='red', linestyle='--', label=f'Mean: {np.mean(data_with_outliers):.1f}')\n",
    "axes[0, 0].axvline(x=np.median(data_with_outliers), color='green', linestyle='--', label=f'Median: {np.median(data_with_outliers):.1f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# StandardScaler\n",
    "standard_scaled = StandardScaler().fit_transform(data_with_outliers.reshape(-1, 1)).flatten()\n",
    "axes[0, 1].hist(standard_scaled, bins=50, color='steelblue', edgecolor='black')\n",
    "axes[0, 1].set_title('StandardScaler (affected by outliers)', fontsize=12)\n",
    "axes[0, 1].axvline(x=0, color='red', linestyle='--', label='Mean=0')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# MinMaxScaler\n",
    "minmax_scaled = MinMaxScaler().fit_transform(data_with_outliers.reshape(-1, 1)).flatten()\n",
    "axes[1, 0].hist(minmax_scaled, bins=50, color='coral', edgecolor='black')\n",
    "axes[1, 0].set_title('MinMaxScaler (compressed by outliers)', fontsize=12)\n",
    "axes[1, 0].axvline(x=0.5, color='red', linestyle='--', label='Middle=0.5')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# RobustScaler\n",
    "robust_scaled = RobustScaler().fit_transform(data_with_outliers.reshape(-1, 1)).flatten()\n",
    "axes[1, 1].hist(robust_scaled, bins=50, color='seagreen', edgecolor='black')\n",
    "axes[1, 1].set_title('RobustScaler (handles outliers well)', fontsize=12)\n",
    "axes[1, 1].axvline(x=0, color='red', linestyle='--', label='Median=0')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.suptitle('Comparison of Scaling Methods with Outliers', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 When to Use Which Scaling Method?\n",
    "\n",
    "| Situation | Recommended Scaler | Reason |\n",
    "|-----------|-------------------|--------|\n",
    "| Normally distributed data | StandardScaler | Preserves distribution shape |\n",
    "| Unknown distribution | StandardScaler | General purpose |\n",
    "| Neural networks | MinMaxScaler | Bounded input values work better |\n",
    "| Image data | MinMaxScaler | Pixel values bounded [0,1] |\n",
    "| Data with many outliers | RobustScaler | Not affected by extreme values |\n",
    "| Sparse data | MinMaxScaler | Preserves sparsity |\n",
    "| Distance-based algorithms | StandardScaler | Equal weight to all features |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive comparison summary\n",
    "comparison_data = sample_data[numerical_features].copy()\n",
    "\n",
    "scalers = {\n",
    "    'StandardScaler': StandardScaler(),\n",
    "    'MinMaxScaler': MinMaxScaler(),\n",
    "    'RobustScaler': RobustScaler()\n",
    "}\n",
    "\n",
    "print(\"Scaling Comparison Summary\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "    scaled_data = scaler.fit_transform(comparison_data)\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Mean range: [{scaled_data.mean(axis=0).min():.4f}, {scaled_data.mean(axis=0).max():.4f}]\")\n",
    "    print(f\"  Std range: [{scaled_data.std(axis=0).min():.4f}, {scaled_data.std(axis=0).max():.4f}]\")\n",
    "    print(f\"  Value range: [{scaled_data.min():.4f}, {scaled_data.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Encoding Categorical Variables\n",
    "\n",
    "Machine learning algorithms require numerical input. Categorical variables must be converted to numbers.\n",
    "\n",
    "### Types of Categorical Variables\n",
    "\n",
    "1. **Nominal**: No inherent order (e.g., colors, cities, names)\n",
    "2. **Ordinal**: Natural order exists (e.g., ratings, education levels)\n",
    "\n",
    "### 3.1 Label Encoding\n",
    "\n",
    "Assigns a unique integer to each category.\n",
    "\n",
    "**Example:**\n",
    "- Red → 0\n",
    "- Green → 1\n",
    "- Blue → 2\n",
    "\n",
    "**When to use:**\n",
    "- Ordinal variables (order matters)\n",
    "- Tree-based algorithms (can handle encoded values)\n",
    "\n",
    "**When NOT to use:**\n",
    "- Nominal variables with linear models (implies false ordering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding Example\n",
    "print(\"Label Encoding Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Original categories\n",
    "print(\"\\nOriginal 'city' values:\")\n",
    "print(sample_data['city'].value_counts())\n",
    "\n",
    "# Apply Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "city_encoded = label_encoder.fit_transform(sample_data['city'])\n",
    "\n",
    "print(\"\\nLabel Encoding Mapping:\")\n",
    "for i, class_name in enumerate(label_encoder.classes_):\n",
    "    print(f\"  {class_name} → {i}\")\n",
    "\n",
    "# Show before and after\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original': sample_data['city'].head(10),\n",
    "    'Label_Encoded': city_encoded[:10]\n",
    "})\n",
    "print(\"\\nSample comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual Label Encoding Implementation\n",
    "def label_encode_manual(series):\n",
    "    \"\"\"\n",
    "    Manually implement label encoding\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Categorical series to encode\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    encoded : np.array\n",
    "        Integer-encoded values\n",
    "    mapping : dict\n",
    "        Category to integer mapping\n",
    "    \"\"\"\n",
    "    unique_values = sorted(series.unique())\n",
    "    mapping = {val: idx for idx, val in enumerate(unique_values)}\n",
    "    encoded = series.map(mapping).values\n",
    "    return encoded, mapping\n",
    "\n",
    "# Test manual implementation\n",
    "city_encoded_manual, city_mapping = label_encode_manual(sample_data['city'])\n",
    "print(\"Manual Label Encoding Mapping:\")\n",
    "for k, v in city_mapping.items():\n",
    "    print(f\"  {k} → {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 One-Hot Encoding\n",
    "\n",
    "Creates binary columns for each category.\n",
    "\n",
    "**Example:**\n",
    "| Original | Red | Green | Blue |\n",
    "|----------|-----|-------|------|\n",
    "| Red      | 1   | 0     | 0    |\n",
    "| Green    | 0   | 1     | 0    |\n",
    "| Blue     | 0   | 0     | 1    |\n",
    "\n",
    "**When to use:**\n",
    "- Nominal variables (no order)\n",
    "- Linear models, neural networks\n",
    "- When categories don't have inherent ranking\n",
    "\n",
    "**Considerations:**\n",
    "- High cardinality = many columns (dimensionality explosion)\n",
    "- May need to drop one column (dummy variable trap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding with pandas (most common method)\n",
    "print(\"One-Hot Encoding with pandas get_dummies()\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Method 1: pd.get_dummies()\n",
    "city_onehot_pandas = pd.get_dummies(sample_data['city'], prefix='city')\n",
    "\n",
    "print(\"\\nOriginal shape:\", sample_data['city'].shape)\n",
    "print(\"One-Hot encoded shape:\", city_onehot_pandas.shape)\n",
    "print(\"\\nNew columns created:\")\n",
    "print(city_onehot_pandas.columns.tolist())\n",
    "print(\"\\nSample one-hot encoded data:\")\n",
    "print(city_onehot_pandas.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot Encoding with scikit-learn\n",
    "print(\"One-Hot Encoding with sklearn OneHotEncoder\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False, drop=None)  # drop='first' for dummy encoding\n",
    "city_onehot_sklearn = onehot_encoder.fit_transform(sample_data[['city']])\n",
    "\n",
    "# Get feature names\n",
    "feature_names = onehot_encoder.get_feature_names_out(['city'])\n",
    "print(\"\\nFeature names:\", feature_names)\n",
    "\n",
    "# Create DataFrame\n",
    "city_onehot_df = pd.DataFrame(city_onehot_sklearn, columns=feature_names)\n",
    "print(\"\\nSample data:\")\n",
    "print(city_onehot_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual One-Hot Encoding Implementation\n",
    "def onehot_encode_manual(series):\n",
    "    \"\"\"\n",
    "    Manually implement one-hot encoding\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    series : pd.Series\n",
    "        Categorical series to encode\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    onehot_df : pd.DataFrame\n",
    "        One-hot encoded DataFrame\n",
    "    \"\"\"\n",
    "    unique_values = sorted(series.unique())\n",
    "    onehot_dict = {}\n",
    "    \n",
    "    for val in unique_values:\n",
    "        col_name = f\"{series.name}_{val}\"\n",
    "        onehot_dict[col_name] = (series == val).astype(int)\n",
    "    \n",
    "    return pd.DataFrame(onehot_dict)\n",
    "\n",
    "# Test manual implementation\n",
    "city_onehot_manual = onehot_encode_manual(sample_data['city'])\n",
    "print(\"Manual One-Hot Encoding:\")\n",
    "print(city_onehot_manual.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling the Dummy Variable Trap\n",
    "print(\"Dummy Variable Trap\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "The Dummy Variable Trap occurs when one-hot encoded columns are \n",
    "perfectly multicollinear (one can be predicted from others).\n",
    "\n",
    "For k categories, we only need k-1 binary columns because the \n",
    "last category can be inferred when all others are 0.\n",
    "\n",
    "Solution: Drop one column (use drop='first' or drop='if_binary')\n",
    "\"\"\")\n",
    "\n",
    "# Demonstrate with drop='first'\n",
    "city_dummy = pd.get_dummies(sample_data['city'], prefix='city', drop_first=True)\n",
    "print(\"\\nWith drop_first=True:\")\n",
    "print(f\"Original categories: {sample_data['city'].nunique()}\")\n",
    "print(f\"Dummy columns: {city_dummy.shape[1]}\")\n",
    "print(f\"Columns: {city_dummy.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Ordinal Encoding\n",
    "\n",
    "Similar to label encoding, but explicitly handles the order of categories.\n",
    "\n",
    "**Use Case:** When categories have a meaningful order (e.g., education levels, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Encoding for ordered categories\n",
    "print(\"Ordinal Encoding Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Education has a natural order: High School < Bachelor < Master < PhD\n",
    "education_order = ['High School', 'Bachelor', 'Master', 'PhD']\n",
    "\n",
    "print(\"\\nOriginal education distribution:\")\n",
    "print(sample_data['education'].value_counts())\n",
    "\n",
    "# Using sklearn OrdinalEncoder\n",
    "ordinal_encoder = OrdinalEncoder(categories=[education_order])\n",
    "education_ordinal = ordinal_encoder.fit_transform(sample_data[['education']])\n",
    "\n",
    "print(\"\\nOrdinal Encoding Mapping:\")\n",
    "for i, edu in enumerate(education_order):\n",
    "    print(f\"  {edu} → {i}\")\n",
    "\n",
    "# Compare\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original': sample_data['education'].head(15),\n",
    "    'Ordinal_Encoded': education_ordinal[:15].flatten()\n",
    "})\n",
    "print(\"\\nSample comparison:\")\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal Encoding for Rating (another example)\n",
    "rating_order = ['Poor', 'Average', 'Good', 'Excellent']\n",
    "\n",
    "ordinal_encoder_rating = OrdinalEncoder(categories=[rating_order])\n",
    "rating_ordinal = ordinal_encoder_rating.fit_transform(sample_data[['rating']])\n",
    "\n",
    "print(\"Rating Ordinal Encoding:\")\n",
    "for i, rating in enumerate(rating_order):\n",
    "    print(f\"  {rating} → {i}\")\n",
    "\n",
    "# Create a comprehensive encoding summary\n",
    "encoding_summary = pd.DataFrame({\n",
    "    'education_original': sample_data['education'],\n",
    "    'education_ordinal': education_ordinal.flatten(),\n",
    "    'rating_original': sample_data['rating'],\n",
    "    'rating_ordinal': rating_ordinal.flatten()\n",
    "})\n",
    "\n",
    "print(\"\\nEncoding Summary (first 10 rows):\")\n",
    "print(encoding_summary.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Target Encoding (Mean Encoding)\n",
    "\n",
    "Encodes categories using the target variable's mean for each category.\n",
    "\n",
    "**Advantages:**\n",
    "- Captures relationship between category and target\n",
    "- Single column (no dimensionality explosion)\n",
    "- Works well with high-cardinality features\n",
    "\n",
    "**Disadvantages:**\n",
    "- Risk of target leakage (requires careful validation split)\n",
    "- Needs smoothing for rare categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Encoding Example\n",
    "print(\"Target Encoding Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a target variable based on income\n",
    "sample_data['high_value_customer'] = (sample_data['income'] > sample_data['income'].median()).astype(int)\n",
    "\n",
    "# Calculate target mean for each city\n",
    "city_target_mean = sample_data.groupby('city')['high_value_customer'].mean()\n",
    "print(\"\\nTarget Mean by City:\")\n",
    "print(city_target_mean.sort_values(ascending=False))\n",
    "\n",
    "# Apply target encoding\n",
    "sample_data['city_target_encoded'] = sample_data['city'].map(city_target_mean)\n",
    "\n",
    "print(\"\\nSample with Target Encoding:\")\n",
    "print(sample_data[['city', 'city_target_encoded', 'high_value_customer']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Encoding with Smoothing (to handle rare categories)\n",
    "def target_encode_with_smoothing(df, category_col, target_col, smoothing=10):\n",
    "    \"\"\"\n",
    "    Target encoding with smoothing to handle rare categories\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    category_col : str\n",
    "        Name of categorical column\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    smoothing : int\n",
    "        Smoothing factor (higher = more regularization)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    encoded : pd.Series\n",
    "        Target-encoded values with smoothing\n",
    "    \"\"\"\n",
    "    global_mean = df[target_col].mean()\n",
    "    \n",
    "    # Calculate category stats\n",
    "    agg = df.groupby(category_col)[target_col].agg(['mean', 'count'])\n",
    "    \n",
    "    # Apply smoothing formula:\n",
    "    # smoothed_mean = (count * category_mean + smoothing * global_mean) / (count + smoothing)\n",
    "    smoothed_mean = (agg['count'] * agg['mean'] + smoothing * global_mean) / (agg['count'] + smoothing)\n",
    "    \n",
    "    return df[category_col].map(smoothed_mean), smoothed_mean\n",
    "\n",
    "# Apply smoothed target encoding\n",
    "city_smoothed, smoothed_mapping = target_encode_with_smoothing(\n",
    "    sample_data, 'city', 'high_value_customer', smoothing=10\n",
    ")\n",
    "\n",
    "print(\"Target Encoding with Smoothing:\")\n",
    "print(\"\\nRaw vs Smoothed means:\")\n",
    "comparison = pd.DataFrame({\n",
    "    'Raw Mean': city_target_mean,\n",
    "    'Smoothed Mean': smoothed_mapping\n",
    "})\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Method Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of encoding methods\n",
    "print(\"Encoding Methods Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "encoding_comparison = pd.DataFrame({\n",
    "    'Method': ['Label Encoding', 'One-Hot Encoding', 'Ordinal Encoding', 'Target Encoding'],\n",
    "    'Best For': ['Tree models, ordinal data', 'Linear models, nominal data', \n",
    "                 'Ordered categories', 'High cardinality'],\n",
    "    'Creates Columns': ['1', 'k (or k-1)', '1', '1'],\n",
    "    'Preserves Order': ['No', 'No', 'Yes', 'No'],\n",
    "    'Risk': ['False ordering', 'Dimensionality', 'None', 'Target leakage']\n",
    "})\n",
    "\n",
    "print(encoding_comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Creation\n",
    "\n",
    "Creating new features from existing ones can significantly improve model performance.\n",
    "\n",
    "### 4.1 Polynomial Features\n",
    "\n",
    "Creates polynomial combinations of features up to a specified degree.\n",
    "\n",
    "For features [a, b] with degree=2:\n",
    "- Output: [1, a, b, a², ab, b²]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Features Example\n",
    "print(\"Polynomial Features Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Sample data\n",
    "simple_data = pd.DataFrame({\n",
    "    'x1': [1, 2, 3, 4, 5],\n",
    "    'x2': [2, 4, 6, 8, 10]\n",
    "})\n",
    "\n",
    "print(\"Original Features:\")\n",
    "print(simple_data)\n",
    "\n",
    "# Create polynomial features (degree=2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(simple_data)\n",
    "poly_names = poly.get_feature_names_out(['x1', 'x2'])\n",
    "\n",
    "poly_df = pd.DataFrame(poly_features, columns=poly_names)\n",
    "print(\"\\nPolynomial Features (degree=2):\")\n",
    "print(poly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual polynomial feature creation\n",
    "def create_polynomial_features_manual(df, features, degree=2):\n",
    "    \"\"\"\n",
    "    Manually create polynomial features\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    features : list\n",
    "        List of feature names to use\n",
    "    degree : int\n",
    "        Maximum polynomial degree\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    result_df : pd.DataFrame\n",
    "        DataFrame with polynomial features added\n",
    "    \"\"\"\n",
    "    result = df[features].copy()\n",
    "    \n",
    "    # Add squared terms\n",
    "    for feat in features:\n",
    "        result[f'{feat}^2'] = df[feat] ** 2\n",
    "        if degree >= 3:\n",
    "            result[f'{feat}^3'] = df[feat] ** 3\n",
    "    \n",
    "    # Add interaction terms\n",
    "    for i, feat1 in enumerate(features):\n",
    "        for feat2 in features[i+1:]:\n",
    "            result[f'{feat1}*{feat2}'] = df[feat1] * df[feat2]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test manual implementation\n",
    "poly_manual = create_polynomial_features_manual(simple_data, ['x1', 'x2'], degree=2)\n",
    "print(\"Manual Polynomial Features:\")\n",
    "print(poly_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Interaction Features\n",
    "\n",
    "Capture relationships between multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction Features\n",
    "print(\"Interaction Features Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Using our sample data\n",
    "interaction_df = sample_data[['age', 'income', 'transaction_amount']].copy()\n",
    "\n",
    "# Create interaction features\n",
    "interaction_df['age_income_interaction'] = interaction_df['age'] * interaction_df['income']\n",
    "interaction_df['income_per_age'] = interaction_df['income'] / interaction_df['age']\n",
    "interaction_df['transaction_to_income_ratio'] = interaction_df['transaction_amount'] / interaction_df['income']\n",
    "\n",
    "print(\"Original + Interaction Features:\")\n",
    "print(interaction_df.head(10))\n",
    "print(\"\\nNew Features Statistics:\")\n",
    "print(interaction_df[['age_income_interaction', 'income_per_age', 'transaction_to_income_ratio']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Aggregation Features\n",
    "\n",
    "Create summary statistics at different granularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation Features\n",
    "print(\"Aggregation Features Demonstration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Calculate aggregations by city\n",
    "city_agg = sample_data.groupby('city').agg({\n",
    "    'income': ['mean', 'median', 'std', 'min', 'max'],\n",
    "    'transaction_amount': ['mean', 'sum', 'count']\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "city_agg.columns = ['_'.join(col).strip() if col[1] else col[0] for col in city_agg.columns]\n",
    "city_agg = city_agg.rename(columns={'city_': 'city'})\n",
    "\n",
    "print(\"City-level Aggregation Features:\")\n",
    "print(city_agg)\n",
    "\n",
    "# Merge back to original data\n",
    "sample_with_agg = sample_data.merge(city_agg, on='city', how='left')\n",
    "\n",
    "# Create relative features\n",
    "sample_with_agg['income_vs_city_avg'] = sample_with_agg['income'] / sample_with_agg['income_mean']\n",
    "sample_with_agg['income_city_percentile'] = sample_with_agg.groupby('city')['income'].transform(\n",
    "    lambda x: x.rank(pct=True)\n",
    ")\n",
    "\n",
    "print(\"\\nRelative Features:\")\n",
    "print(sample_with_agg[['city', 'income', 'income_mean', 'income_vs_city_avg', 'income_city_percentile']].head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Domain-Specific Features\n",
    "\n",
    "Features created based on domain knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-Specific Features for Customer Data\n",
    "print(\"Domain-Specific Features\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Age-based features\n",
    "sample_data['age_group'] = pd.cut(sample_data['age'], \n",
    "                                   bins=[0, 25, 35, 45, 55, 65, 100],\n",
    "                                   labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
    "\n",
    "# Income-based features\n",
    "sample_data['income_bracket'] = pd.qcut(sample_data['income'], q=5, \n",
    "                                         labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Transaction patterns\n",
    "sample_data['is_high_spender'] = (sample_data['transaction_amount'] > \n",
    "                                   sample_data['transaction_amount'].quantile(0.75)).astype(int)\n",
    "\n",
    "# Log transformations for skewed distributions\n",
    "sample_data['log_income'] = np.log1p(sample_data['income'])\n",
    "sample_data['log_transaction'] = np.log1p(sample_data['transaction_amount'])\n",
    "\n",
    "print(\"New Domain Features:\")\n",
    "print(sample_data[['age', 'age_group', 'income', 'income_bracket', \n",
    "                   'transaction_amount', 'is_high_spender', 'log_income']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize log transformation effect\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Original income distribution\n",
    "axes[0, 0].hist(sample_data['income'], bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Original Income Distribution (Skewed)', fontsize=12)\n",
    "axes[0, 0].set_xlabel('Income')\n",
    "\n",
    "# Log-transformed income\n",
    "axes[0, 1].hist(sample_data['log_income'], bins=50, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('Log-Transformed Income (More Normal)', fontsize=12)\n",
    "axes[0, 1].set_xlabel('Log(Income)')\n",
    "\n",
    "# Original transaction distribution\n",
    "axes[1, 0].hist(sample_data['transaction_amount'], bins=50, color='salmon', edgecolor='black')\n",
    "axes[1, 0].set_title('Original Transaction Distribution (Skewed)', fontsize=12)\n",
    "axes[1, 0].set_xlabel('Transaction Amount')\n",
    "\n",
    "# Log-transformed transaction\n",
    "axes[1, 1].hist(sample_data['log_transaction'], bins=50, color='plum', edgecolor='black')\n",
    "axes[1, 1].set_title('Log-Transformed Transaction (More Normal)', fontsize=12)\n",
    "axes[1, 1].set_xlabel('Log(Transaction)')\n",
    "\n",
    "plt.suptitle('Effect of Log Transformation on Skewed Distributions', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Handling DateTime Features\n",
    "\n",
    "DateTime features contain rich information that can be extracted for ML models.\n",
    "\n",
    "### Common DateTime Extractions:\n",
    "- Year, Month, Day\n",
    "- Hour, Minute, Second\n",
    "- Day of Week\n",
    "- Week of Year\n",
    "- Quarter\n",
    "- Is Weekend\n",
    "- Time Since Event\n",
    "- Cyclical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DateTime Feature Engineering\n",
    "print(\"DateTime Feature Engineering\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a copy with datetime features\n",
    "datetime_df = sample_data[['purchase_date', 'transaction_amount']].copy()\n",
    "\n",
    "# Ensure datetime type\n",
    "datetime_df['purchase_date'] = pd.to_datetime(datetime_df['purchase_date'])\n",
    "\n",
    "print(\"Original DateTime:\")\n",
    "print(datetime_df['purchase_date'].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract basic datetime components\n",
    "datetime_df['year'] = datetime_df['purchase_date'].dt.year\n",
    "datetime_df['month'] = datetime_df['purchase_date'].dt.month\n",
    "datetime_df['day'] = datetime_df['purchase_date'].dt.day\n",
    "datetime_df['hour'] = datetime_df['purchase_date'].dt.hour\n",
    "datetime_df['minute'] = datetime_df['purchase_date'].dt.minute\n",
    "\n",
    "# Additional datetime features\n",
    "datetime_df['day_of_week'] = datetime_df['purchase_date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "datetime_df['day_name'] = datetime_df['purchase_date'].dt.day_name()\n",
    "datetime_df['week_of_year'] = datetime_df['purchase_date'].dt.isocalendar().week\n",
    "datetime_df['quarter'] = datetime_df['purchase_date'].dt.quarter\n",
    "datetime_df['is_weekend'] = (datetime_df['day_of_week'] >= 5).astype(int)\n",
    "datetime_df['is_month_start'] = datetime_df['purchase_date'].dt.is_month_start.astype(int)\n",
    "datetime_df['is_month_end'] = datetime_df['purchase_date'].dt.is_month_end.astype(int)\n",
    "\n",
    "print(\"Extracted DateTime Features:\")\n",
    "print(datetime_df.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based features\n",
    "reference_date = datetime_df['purchase_date'].max()\n",
    "datetime_df['days_since_purchase'] = (reference_date - datetime_df['purchase_date']).dt.days\n",
    "\n",
    "# Part of day\n",
    "def get_part_of_day(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "datetime_df['part_of_day'] = datetime_df['hour'].apply(get_part_of_day)\n",
    "\n",
    "print(\"Additional Time Features:\")\n",
    "print(datetime_df[['purchase_date', 'hour', 'part_of_day', 'days_since_purchase']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cyclical Encoding for DateTime Features\n",
    "print(\"Cyclical Encoding for DateTime Features\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Problem with simple numerical encoding:\n",
    "- Hour 23 and Hour 0 are far apart numerically (23 - 0 = 23)\n",
    "- But they are actually close in time (1 hour apart)\n",
    "\n",
    "Solution: Cyclical encoding using sine and cosine\n",
    "- Maps cyclical values to a circle\n",
    "- Hour 23 and Hour 0 become adjacent on the circle\n",
    "\"\"\")\n",
    "\n",
    "def cyclical_encode(df, col, max_val):\n",
    "    \"\"\"\n",
    "    Encode cyclical features using sin/cos transformation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    col : str\n",
    "        Column name to encode\n",
    "    max_val : int\n",
    "        Maximum value in the cycle (e.g., 24 for hours, 7 for days)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    sin_col, cos_col : pd.Series\n",
    "        Sine and cosine encoded values\n",
    "    \"\"\"\n",
    "    sin_col = np.sin(2 * np.pi * df[col] / max_val)\n",
    "    cos_col = np.cos(2 * np.pi * df[col] / max_val)\n",
    "    return sin_col, cos_col\n",
    "\n",
    "# Apply cyclical encoding\n",
    "datetime_df['hour_sin'], datetime_df['hour_cos'] = cyclical_encode(datetime_df, 'hour', 24)\n",
    "datetime_df['dow_sin'], datetime_df['dow_cos'] = cyclical_encode(datetime_df, 'day_of_week', 7)\n",
    "datetime_df['month_sin'], datetime_df['month_cos'] = cyclical_encode(datetime_df, 'month', 12)\n",
    "\n",
    "print(\"Cyclical Encoded Features:\")\n",
    "print(datetime_df[['hour', 'hour_sin', 'hour_cos', 'day_of_week', 'dow_sin', 'dow_cos']].head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cyclical encoding\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Hour encoding\n",
    "hours = np.arange(24)\n",
    "hour_sin = np.sin(2 * np.pi * hours / 24)\n",
    "hour_cos = np.cos(2 * np.pi * hours / 24)\n",
    "\n",
    "axes[0].scatter(hour_cos, hour_sin, c=hours, cmap='viridis', s=100)\n",
    "for i, hour in enumerate(hours):\n",
    "    axes[0].annotate(str(hour), (hour_cos[i], hour_sin[i]), fontsize=8)\n",
    "axes[0].set_xlabel('Cosine')\n",
    "axes[0].set_ylabel('Sine')\n",
    "axes[0].set_title('Hour Cyclical Encoding', fontsize=12)\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Day of week encoding\n",
    "days = np.arange(7)\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "day_sin = np.sin(2 * np.pi * days / 7)\n",
    "day_cos = np.cos(2 * np.pi * days / 7)\n",
    "\n",
    "axes[1].scatter(day_cos, day_sin, c=days, cmap='tab10', s=100)\n",
    "for i, day in enumerate(day_names):\n",
    "    axes[1].annotate(day, (day_cos[i], day_sin[i]), fontsize=8)\n",
    "axes[1].set_xlabel('Cosine')\n",
    "axes[1].set_ylabel('Sine')\n",
    "axes[1].set_title('Day of Week Cyclical Encoding', fontsize=12)\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "# Month encoding\n",
    "months = np.arange(1, 13)\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "month_sin = np.sin(2 * np.pi * months / 12)\n",
    "month_cos = np.cos(2 * np.pi * months / 12)\n",
    "\n",
    "axes[2].scatter(month_cos, month_sin, c=months, cmap='hsv', s=100)\n",
    "for i, month in enumerate(month_names):\n",
    "    axes[2].annotate(month, (month_cos[i], month_sin[i]), fontsize=8)\n",
    "axes[2].set_xlabel('Cosine')\n",
    "axes[2].set_ylabel('Sine')\n",
    "axes[2].set_title('Month Cyclical Encoding', fontsize=12)\n",
    "axes[2].set_aspect('equal')\n",
    "\n",
    "plt.suptitle('Cyclical Encoding Visualization', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete DateTime Feature Engineering Function\n",
    "def engineer_datetime_features(df, datetime_col, reference_date=None):\n",
    "    \"\"\"\n",
    "    Comprehensive datetime feature engineering\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame\n",
    "    datetime_col : str\n",
    "        Name of datetime column\n",
    "    reference_date : datetime, optional\n",
    "        Reference date for calculating time differences\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    result_df : pd.DataFrame\n",
    "        DataFrame with engineered datetime features\n",
    "    \"\"\"\n",
    "    result = pd.DataFrame()\n",
    "    dt = pd.to_datetime(df[datetime_col])\n",
    "    \n",
    "    # Basic extractions\n",
    "    result[f'{datetime_col}_year'] = dt.dt.year\n",
    "    result[f'{datetime_col}_month'] = dt.dt.month\n",
    "    result[f'{datetime_col}_day'] = dt.dt.day\n",
    "    result[f'{datetime_col}_hour'] = dt.dt.hour\n",
    "    result[f'{datetime_col}_dayofweek'] = dt.dt.dayofweek\n",
    "    result[f'{datetime_col}_quarter'] = dt.dt.quarter\n",
    "    result[f'{datetime_col}_weekofyear'] = dt.dt.isocalendar().week.astype(int)\n",
    "    \n",
    "    # Boolean features\n",
    "    result[f'{datetime_col}_is_weekend'] = (dt.dt.dayofweek >= 5).astype(int)\n",
    "    result[f'{datetime_col}_is_month_start'] = dt.dt.is_month_start.astype(int)\n",
    "    result[f'{datetime_col}_is_month_end'] = dt.dt.is_month_end.astype(int)\n",
    "    \n",
    "    # Cyclical encoding\n",
    "    result[f'{datetime_col}_hour_sin'] = np.sin(2 * np.pi * dt.dt.hour / 24)\n",
    "    result[f'{datetime_col}_hour_cos'] = np.cos(2 * np.pi * dt.dt.hour / 24)\n",
    "    result[f'{datetime_col}_dow_sin'] = np.sin(2 * np.pi * dt.dt.dayofweek / 7)\n",
    "    result[f'{datetime_col}_dow_cos'] = np.cos(2 * np.pi * dt.dt.dayofweek / 7)\n",
    "    result[f'{datetime_col}_month_sin'] = np.sin(2 * np.pi * dt.dt.month / 12)\n",
    "    result[f'{datetime_col}_month_cos'] = np.cos(2 * np.pi * dt.dt.month / 12)\n",
    "    \n",
    "    # Time since reference\n",
    "    if reference_date is None:\n",
    "        reference_date = dt.max()\n",
    "    result[f'{datetime_col}_days_since'] = (reference_date - dt).dt.days\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test the function\n",
    "datetime_features = engineer_datetime_features(sample_data, 'purchase_date')\n",
    "print(\"Complete DateTime Feature Engineering:\")\n",
    "print(f\"Number of features created: {datetime_features.shape[1]}\")\n",
    "print(f\"\\nFeatures: {datetime_features.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Assignment: Transform Raw Dataset to ML-Ready Format\n",
    "\n",
    "### Task\n",
    "Transform a raw dataset with mixed data types (numerical, categorical, datetime) into a machine learning-ready format.\n",
    "\n",
    "### Dataset\n",
    "We'll use a synthetic e-commerce dataset with:\n",
    "- Customer information\n",
    "- Purchase history\n",
    "- Mixed data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the raw e-commerce dataset\n",
    "np.random.seed(42)\n",
    "n_samples = 2000\n",
    "\n",
    "raw_data = pd.DataFrame({\n",
    "    # Numerical features\n",
    "    'customer_age': np.random.randint(18, 75, n_samples),\n",
    "    'annual_income': np.random.exponential(60000, n_samples) + 25000,\n",
    "    'account_age_days': np.random.randint(1, 3650, n_samples),\n",
    "    'num_purchases': np.random.poisson(15, n_samples),\n",
    "    'avg_purchase_value': np.random.exponential(80, n_samples) + 20,\n",
    "    'website_visits': np.random.poisson(50, n_samples),\n",
    "    'cart_abandonment_rate': np.random.beta(2, 5, n_samples),\n",
    "    \n",
    "    # Categorical features\n",
    "    'gender': np.random.choice(['Male', 'Female', 'Other'], n_samples, p=[0.48, 0.48, 0.04]),\n",
    "    'membership_type': np.random.choice(['Basic', 'Silver', 'Gold', 'Platinum'], n_samples,\n",
    "                                        p=[0.4, 0.3, 0.2, 0.1]),\n",
    "    'preferred_category': np.random.choice(['Electronics', 'Fashion', 'Home', 'Sports', 'Books'], n_samples),\n",
    "    'payment_method': np.random.choice(['Credit Card', 'Debit Card', 'PayPal', 'Cash'], n_samples),\n",
    "    'device_type': np.random.choice(['Desktop', 'Mobile', 'Tablet'], n_samples, p=[0.4, 0.5, 0.1]),\n",
    "    'customer_segment': np.random.choice(['New', 'Regular', 'VIP', 'At-Risk'], n_samples,\n",
    "                                         p=[0.25, 0.45, 0.15, 0.15]),\n",
    "    \n",
    "    # DateTime features\n",
    "    'registration_date': pd.date_range('2019-01-01', periods=n_samples, freq='4H') + \n",
    "                         pd.to_timedelta(np.random.randint(0, 30, n_samples), unit='D'),\n",
    "    'last_purchase_date': pd.date_range('2023-06-01', periods=n_samples, freq='30T') +\n",
    "                          pd.to_timedelta(np.random.randint(0, 180, n_samples), unit='D'),\n",
    "    \n",
    "    # Target variable (will the customer make a purchase in next 30 days?)\n",
    "    'will_purchase': np.random.choice([0, 1], n_samples, p=[0.6, 0.4])\n",
    "})\n",
    "\n",
    "# Add some missing values for realism\n",
    "missing_indices = np.random.choice(n_samples, 100, replace=False)\n",
    "raw_data.loc[missing_indices[:50], 'annual_income'] = np.nan\n",
    "raw_data.loc[missing_indices[50:], 'cart_abandonment_rate'] = np.nan\n",
    "\n",
    "print(\"Raw E-Commerce Dataset\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Shape: {raw_data.shape}\")\n",
    "print(f\"\\nColumn Types:\\n{raw_data.dtypes}\")\n",
    "print(f\"\\nMissing Values:\\n{raw_data.isnull().sum()[raw_data.isnull().sum() > 0]}\")\n",
    "print(\"\\nFirst 10 rows:\")\n",
    "raw_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the raw data\n",
    "print(\"Raw Data Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identify column types\n",
    "numerical_cols = raw_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = raw_data.select_dtypes(include=['object']).columns.tolist()\n",
    "datetime_cols = raw_data.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(f\"\\nCategorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"\\nDatetime columns ({len(datetime_cols)}): {datetime_cols}\")\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nNumerical Statistics:\")\n",
    "print(raw_data[numerical_cols].describe().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical value counts\n",
    "print(\"Categorical Value Distributions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(raw_data[col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Handling Missing Values\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a copy for transformation\n",
    "df = raw_data.copy()\n",
    "\n",
    "# Fill numerical missing values with median\n",
    "for col in ['annual_income', 'cart_abandonment_rate']:\n",
    "    median_val = df[col].median()\n",
    "    df[col].fillna(median_val, inplace=True)\n",
    "    print(f\"Filled {col} missing values with median: {median_val:.2f}\")\n",
    "\n",
    "print(f\"\\nMissing values after handling: {df.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Feature Engineering - Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new numerical features\n",
    "print(\"Creating New Numerical Features\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Derived features\n",
    "df['total_spending'] = df['num_purchases'] * df['avg_purchase_value']\n",
    "df['purchase_frequency'] = df['num_purchases'] / (df['account_age_days'] / 30)  # purchases per month\n",
    "df['visits_per_purchase'] = df['website_visits'] / (df['num_purchases'] + 1)  # +1 to avoid division by 0\n",
    "df['income_per_purchase'] = df['annual_income'] / (df['num_purchases'] + 1)\n",
    "\n",
    "# Log transformations for skewed features\n",
    "df['log_annual_income'] = np.log1p(df['annual_income'])\n",
    "df['log_total_spending'] = np.log1p(df['total_spending'])\n",
    "\n",
    "# Binning continuous variables\n",
    "df['age_group'] = pd.cut(df['customer_age'], \n",
    "                         bins=[0, 25, 35, 45, 55, 65, 100],\n",
    "                         labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+'])\n",
    "\n",
    "df['income_bracket'] = pd.qcut(df['annual_income'], q=5, \n",
    "                               labels=['Very_Low', 'Low', 'Medium', 'High', 'Very_High'])\n",
    "\n",
    "print(\"New numerical features created:\")\n",
    "new_num_features = ['total_spending', 'purchase_frequency', 'visits_per_purchase', \n",
    "                    'income_per_purchase', 'log_annual_income', 'log_total_spending']\n",
    "print(df[new_num_features].describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Feature Engineering - DateTime Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer datetime features\n",
    "print(\"Engineering DateTime Features\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reference date (today)\n",
    "reference_date = pd.Timestamp('2024-01-01')\n",
    "\n",
    "# Registration date features\n",
    "df['reg_year'] = df['registration_date'].dt.year\n",
    "df['reg_month'] = df['registration_date'].dt.month\n",
    "df['reg_dayofweek'] = df['registration_date'].dt.dayofweek\n",
    "df['days_since_registration'] = (reference_date - df['registration_date']).dt.days\n",
    "\n",
    "# Last purchase date features\n",
    "df['last_purchase_month'] = df['last_purchase_date'].dt.month\n",
    "df['last_purchase_dayofweek'] = df['last_purchase_date'].dt.dayofweek\n",
    "df['days_since_last_purchase'] = (reference_date - df['last_purchase_date']).dt.days\n",
    "df['last_purchase_is_weekend'] = (df['last_purchase_dayofweek'] >= 5).astype(int)\n",
    "\n",
    "# Cyclical encoding for months\n",
    "df['reg_month_sin'] = np.sin(2 * np.pi * df['reg_month'] / 12)\n",
    "df['reg_month_cos'] = np.cos(2 * np.pi * df['reg_month'] / 12)\n",
    "df['last_purchase_month_sin'] = np.sin(2 * np.pi * df['last_purchase_month'] / 12)\n",
    "df['last_purchase_month_cos'] = np.cos(2 * np.pi * df['last_purchase_month'] / 12)\n",
    "\n",
    "# Recency feature (important for customer behavior)\n",
    "df['recency_score'] = 1 / (df['days_since_last_purchase'] + 1)  # Higher = more recent\n",
    "\n",
    "print(\"DateTime features created:\")\n",
    "datetime_features = ['days_since_registration', 'days_since_last_purchase', \n",
    "                     'last_purchase_is_weekend', 'recency_score']\n",
    "print(df[datetime_features].describe().round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "print(\"Encoding Categorical Variables\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ordinal encoding for ordered categories\n",
    "membership_order = ['Basic', 'Silver', 'Gold', 'Platinum']\n",
    "segment_order = ['New', 'At-Risk', 'Regular', 'VIP']\n",
    "\n",
    "ordinal_encoder_membership = OrdinalEncoder(categories=[membership_order])\n",
    "ordinal_encoder_segment = OrdinalEncoder(categories=[segment_order])\n",
    "\n",
    "df['membership_encoded'] = ordinal_encoder_membership.fit_transform(df[['membership_type']]).flatten()\n",
    "df['segment_encoded'] = ordinal_encoder_segment.fit_transform(df[['customer_segment']]).flatten()\n",
    "\n",
    "print(\"\\nOrdinal Encoding:\")\n",
    "print(f\"Membership: {dict(zip(membership_order, range(4)))}\")\n",
    "print(f\"Segment: {dict(zip(segment_order, range(4)))}\")\n",
    "\n",
    "# One-hot encoding for nominal categories\n",
    "onehot_cols = ['gender', 'preferred_category', 'payment_method', 'device_type', 'age_group', 'income_bracket']\n",
    "\n",
    "df_encoded = pd.get_dummies(df, columns=onehot_cols, drop_first=False)\n",
    "\n",
    "print(f\"\\nOne-Hot encoded columns: {onehot_cols}\")\n",
    "print(f\"Total columns after encoding: {df_encoded.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Scale Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale numerical features\n",
    "print(\"Scaling Numerical Features\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Identify final numerical columns to scale\n",
    "cols_to_scale = [\n",
    "    'customer_age', 'annual_income', 'account_age_days', 'num_purchases',\n",
    "    'avg_purchase_value', 'website_visits', 'cart_abandonment_rate',\n",
    "    'total_spending', 'purchase_frequency', 'visits_per_purchase',\n",
    "    'income_per_purchase', 'log_annual_income', 'log_total_spending',\n",
    "    'days_since_registration', 'days_since_last_purchase', 'recency_score',\n",
    "    'membership_encoded', 'segment_encoded'\n",
    "]\n",
    "\n",
    "# Use StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_encoded[cols_to_scale] = scaler.fit_transform(df_encoded[cols_to_scale])\n",
    "\n",
    "print(f\"Scaled {len(cols_to_scale)} numerical columns\")\n",
    "print(\"\\nScaled features statistics (should have mean~0, std~1):\")\n",
    "print(df_encoded[cols_to_scale].describe().round(2).loc[['mean', 'std']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Prepare Final ML-Ready Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare final dataset\n",
    "print(\"Preparing Final ML-Ready Dataset\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Drop original categorical and datetime columns\n",
    "cols_to_drop = ['membership_type', 'customer_segment', 'registration_date', \n",
    "                'last_purchase_date', 'reg_year', 'reg_month', 'reg_dayofweek',\n",
    "                'last_purchase_month', 'last_purchase_dayofweek']\n",
    "\n",
    "df_final = df_encoded.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "# Separate features and target\n",
    "X = df_final.drop(columns=['will_purchase'])\n",
    "y = df_final['will_purchase']\n",
    "\n",
    "print(f\"Final dataset shape: {df_final.shape}\")\n",
    "print(f\"Features (X) shape: {X.shape}\")\n",
    "print(f\"Target (y) shape: {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\\n{y.value_counts(normalize=True).round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train-Test Split\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining target distribution:\\n{y_train.value_counts(normalize=True).round(3)}\")\n",
    "print(f\"\\nTest target distribution:\\n{y_test.value_counts(normalize=True).round(3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\"\"\n",
    "Original Dataset:\n",
    "  - Samples: {raw_data.shape[0]}\n",
    "  - Features: {raw_data.shape[1]}\n",
    "  - Numerical: {len(numerical_cols)}\n",
    "  - Categorical: {len(categorical_cols)}\n",
    "  - DateTime: {len(datetime_cols)}\n",
    "  - Missing values: {raw_data.isnull().sum().sum()}\n",
    "\n",
    "Final ML-Ready Dataset:\n",
    "  - Samples: {X.shape[0]}\n",
    "  - Features: {X.shape[1]}\n",
    "  - Missing values: {X.isnull().sum().sum()}\n",
    "  - All features are numerical: {X.select_dtypes(include=[np.number]).shape[1] == X.shape[1]}\n",
    "\n",
    "Transformations Applied:\n",
    "  1. Missing value imputation (median)\n",
    "  2. Feature creation (derived features)\n",
    "  3. Log transformations (skewed features)\n",
    "  4. Binning (age groups, income brackets)\n",
    "  5. DateTime feature extraction\n",
    "  6. Cyclical encoding (months)\n",
    "  7. Ordinal encoding (membership, segment)\n",
    "  8. One-hot encoding (gender, category, payment, device)\n",
    "  9. Standard scaling (all numerical features)\n",
    "  10. Train-test split (80-20)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nFeature List:\")\n",
    "for i, col in enumerate(X.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the processed data\n",
    "X_train.to_csv('X_train_processed.csv', index=False)\n",
    "X_test.to_csv('X_test_processed.csv', index=False)\n",
    "y_train.to_csv('y_train.csv', index=False)\n",
    "y_test.to_csv('y_test.csv', index=False)\n",
    "\n",
    "print(\"Processed data saved to CSV files!\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - X_train_processed.csv\")\n",
    "print(\"  - X_test_processed.csv\")\n",
    "print(\"  - y_train.csv\")\n",
    "print(\"  - y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Feature Scaling**\n",
    "   - StandardScaler: Mean=0, Std=1 (best for most cases)\n",
    "   - MinMaxScaler: Range [0,1] (best for bounded data)\n",
    "   - RobustScaler: Based on median/IQR (best for outliers)\n",
    "\n",
    "2. **Categorical Encoding**\n",
    "   - Label Encoding: For ordinal data or tree models\n",
    "   - One-Hot Encoding: For nominal data with linear models\n",
    "   - Target Encoding: For high cardinality features\n",
    "\n",
    "3. **Feature Creation**\n",
    "   - Polynomial features for non-linear relationships\n",
    "   - Interaction features to capture combined effects\n",
    "   - Domain-specific features using business knowledge\n",
    "\n",
    "4. **DateTime Features**\n",
    "   - Extract components (year, month, day, hour)\n",
    "   - Create cyclical encodings for periodic features\n",
    "   - Calculate time-based metrics (recency, age)\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- Always fit scalers on training data only\n",
    "- Handle missing values before scaling\n",
    "- Choose encoding based on data type and model\n",
    "- Create features based on domain knowledge\n",
    "- Document all transformations for reproducibility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
